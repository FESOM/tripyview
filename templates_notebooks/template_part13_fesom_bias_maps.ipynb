{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b00ad0-9b4c-4829-8cb5-dc2d41c8a72b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vname = \"temp\"\n",
    "model_name = \"AWICM-3\"\n",
    "model_path = \"/work/ab0995/a270275/experiments/awicm3test011/outdata/fesom\"\n",
    "reference_path = \"/work/ab0246/a270092/postprocessing/climatologies/fdiag/\"\n",
    "reference_years = \"1985\"\n",
    "model_start = 1990\n",
    "model_end = 1991\n",
    "remap_resolution = \"180x91\"\n",
    "year=[model_start,model_end]\n",
    "#remap_resolution = \"180x91\"\n",
    "mesh_file = \"mesh.nc\"\n",
    "meshpath = \"/work/ab0246/a270092/input/fesom2/core2\"\n",
    "out_path = \"./output/\"\n",
    "dpi = 150\n",
    "tripyview_path=\"/home/a/a270275/tripyview\"\n",
    "tripyview_path=\"/home/a/a270275/tripyview\"\n",
    "# parameters\n",
    "#___Dask Client Parameters____________________________________________________________\n",
    "do_papermill      = False\n",
    "do_parallel       = True\n",
    "parallel_tnprc    = 128                          # total number of available CPUs\n",
    "parallel_nprc     = 72                           # number of dask workers\n",
    "parallel_nprc_bin = parallel_tnprc-parallel_nprc # number of processor used to parallize the binning loop\n",
    "parallel_tmem     = 200                          # max. available RAM\n",
    "\n",
    "#___Mesh Path & Save Path_____________________________________________________________\n",
    "# mesh_path ='/work/ollie/projects/clidyn/FESOM2/meshes/core2/'\n",
    "save_path         = '/work/ab0995/a270275/runconfig/Results/AWICM3_test031' #~/figures/test_papermill/'\n",
    "save_fname        = None # filename from papermill come in through save_fname\n",
    "tripyrun_name     = 'AWICM3_test018' # papermill workflow name of notebook \n",
    "tripyrun_analysis = None # papermill diagnostic driver\n",
    "tripyrun_spath_nb = None # papermill path to processed notebooks\n",
    "tripyrun_spath_fig= None # papermill path to processed figures\n",
    "\n",
    "#___Data Path & Input Names___________________________________________________________\n",
    "input_paths   = list()\n",
    "input_paths.append('/work/ab0995/a270275/experiments/5Ymulti_diag/outdata/fesom/')\n",
    "\n",
    "\n",
    "input_names   = list()\n",
    "input_names.append('Fesom 2.6')\n",
    "input_paths = [\"/work/ab0995/a270275/experiments/awicm3test011/outdata/fesom\"]\n",
    "# n_cycl: which spinupcycle should be plottet if do_allcycl all spinupcycles from [1...n_cycle] are plottet, if None path is directly used\n",
    "n_cycl    = None\n",
    "do_allcycl= False\n",
    "\n",
    "\n",
    "mon, day, record, box, depth = None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac0d00c1-68d3-44cb-8b48-ac7dc7519273",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tripypath /home/a/a270275/tripyview\n"
     ]
    }
   ],
   "source": [
    "# Add the parent directory to sys.path and load config\n",
    "import sys\n",
    "import os\n",
    "import pdb\n",
    "print(\"tripypath\",tripyview_path)\n",
    "sys.path.append(tripyview_path)\n",
    "from config import *\n",
    "\n",
    "model_path = os.path.dirname(model_path)\n",
    "# Mark as started\n",
    "SCRIPT_NAME = \"part13_fesom_bias_maps.ipynb\"   #ADAPT\n",
    "update_status(SCRIPT_NAME, \" Started\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bba6aa55-71d5-49b5-b4fe-bc6becb85930",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Add the parent directory to sys.path and load config\u001b[39;00m\n\u001b[32m      3\u001b[39m saved_filenames=[]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m input_paths = [\u001b[43mmodel_path\u001b[49m+\u001b[33m'\u001b[39m\u001b[33m/fesom\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m input_names = [model_name]\n\u001b[32m      6\u001b[39m exps = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(model_start, model_end+\u001b[32m1\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'model_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Add the parent directory to sys.path and load config\n",
    "\n",
    "saved_filenames=[]\n",
    "input_paths = [model_path+'/fesom']\n",
    "input_names = [model_name]\n",
    "exps = list(range(model_start, model_end+1))\n",
    "years=range(model_start, model_end+1)\n",
    "variable = 'temp'\n",
    "out_path = save_path +  \"/figures\"\n",
    "rowscol=[1,1]\n",
    "bbox = [-180, 180, -80, 90]\n",
    "res = [360, 180]\n",
    "mapproj='pc'\n",
    "figsize=(6, 4.5)\n",
    "\n",
    "levels = [-5, 5, 21]\n",
    "units = r'$^\\circ$C'\n",
    "how=\"mean\"\n",
    "\n",
    "\n",
    "\n",
    "def data_to_plot(plotds, depth):\n",
    "    plot_data = []\n",
    "    plot_names = []\n",
    "    for key, value in plotds[depth].items():\n",
    "        if value['nodiff'] is False:\n",
    "            plot_data.append(value['data'])\n",
    "            plot_names.append(key)\n",
    "                \n",
    "    return plot_data, plot_names\n",
    "\n",
    "# Mean Deviation weighted\n",
    "def md(predictions, targets, wgts):\n",
    "    output_errors = np.average((predictions - targets), axis=0, weights=wgts)\n",
    "    return (output_errors).mean()\n",
    "\n",
    "def get_cmap(cmap=None):\n",
    "    \"\"\"Get the color map.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cmap: str, mpl.colors.Colormap\n",
    "        The colormap can be provided as the name (should be in matplotlib or cmocean colormaps),\n",
    "        or as matplotlib colormap object.\n",
    "    Returns\n",
    "    -------\n",
    "    colormap:mpl.colors.Colormap\n",
    "        Matplotlib colormap object.\n",
    "    \"\"\"\n",
    "    if cmap:\n",
    "        if isinstance(cmap, (mpl.colors.Colormap)):\n",
    "            colormap = cmap\n",
    "        elif cmap in cmof.cmapnames:\n",
    "            colormap = cmo.cmap_d[cmap]\n",
    "        elif cmap in plt.colormaps():\n",
    "            colormap = plt.get_cmap(cmap)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Get unrecognised name for the colormap `{}`. Colormaps should be from standard matplotlib set of from cmocean package.\".format(\n",
    "                    cmap\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        colormap = plt.get_cmap(\"Spectral_r\")\n",
    "\n",
    "    return colormap\n",
    "\n",
    "def interpolate_for_plot(\n",
    "    data,\n",
    "    mesh,\n",
    "    lonreg2,\n",
    "    latreg2,\n",
    "    interp=\"nn\",\n",
    "    distances_path=None,\n",
    "    inds_path=None,\n",
    "    radius_of_influence=None,\n",
    "    basepath=None,\n",
    "    qhull_path=None,\n",
    "):\n",
    "    \"\"\"Interpolate for the plot.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mesh: mesh object\n",
    "        FESOM2 mesh object\n",
    "    data: np.array or list of np.arrays\n",
    "        FESOM 2 data on nodes (for u,v,u_ice and v_ice one have to first interpolate from elements to nodes).\n",
    "        Can be ether one np.ndarray or list of np.ndarrays.\n",
    "    lonreg2: 2D numpy array\n",
    "        Longitudes of the regular grid.\n",
    "    latreg2: 2D numpy array\n",
    "        Latitudes of the regular grid.\n",
    "    interp: str\n",
    "        Interpolation method. Options are 'nn' (nearest neighbor), 'idist' (inverce distance), \"linear\" and \"cubic\".\n",
    "    distances_path : string\n",
    "        Path to the file with distances. If not provided and dumpfile=True, it will be created.\n",
    "    inds_path : string\n",
    "        Path to the file with inds. If not provided and dumpfile=True, it will be created.\n",
    "    qhull_path : str\n",
    "         Path to the file with qhull (needed for linear and cubic interpolations). If not provided and dumpfile=True, it will be created.\n",
    "    basepath: str\n",
    "        path where to store additional interpolation files. If None (default),\n",
    "        the path of the mesh will be used.\n",
    "    \"\"\"\n",
    "    interpolated = []\n",
    "    for datainstance in data:\n",
    "\n",
    "        if interp == \"nn\":\n",
    "            ofesom = fesom2regular(\n",
    "                datainstance,\n",
    "                mesh,\n",
    "                lonreg2,\n",
    "                latreg2,\n",
    "                distances_path=distances_path,\n",
    "                inds_path=inds_path,\n",
    "                radius_of_influence=radius_of_influence,\n",
    "                basepath=basepath,\n",
    "            )\n",
    "            interpolated.append(ofesom)\n",
    "        elif interp == \"idist\":\n",
    "            ofesom = fesom2regular(\n",
    "                datainstance,\n",
    "                mesh,\n",
    "                lonreg2,\n",
    "                latreg2,\n",
    "                distances_path=distances_path,\n",
    "                inds_path=inds_path,\n",
    "                radius_of_influence=radius_of_influence,\n",
    "                how=\"idist\",\n",
    "                k=5,\n",
    "                basepath=basepath,\n",
    "            )\n",
    "            interpolated.append(ofesom)\n",
    "        elif interp == \"linear\":\n",
    "            ofesom = fesom2regular(\n",
    "                datainstance,\n",
    "                mesh,\n",
    "                lonreg2,\n",
    "                latreg2,\n",
    "                how=\"linear\",\n",
    "                qhull_path=qhull_path,\n",
    "                basepath=basepath,\n",
    "            )\n",
    "            interpolated.append(ofesom)\n",
    "        elif interp == \"cubic\":\n",
    "            ofesom = fesom2regular(\n",
    "                datainstance, mesh, lonreg2, latreg2, basepath=basepath, how=\"cubic\"\n",
    "            )\n",
    "            interpolated.append(ofesom)\n",
    "    return interpolated\n",
    "\n",
    "def create_indexes_and_distances(mesh, lons, lats, k=1, n_jobs=2):\n",
    "    \"\"\"\n",
    "    Creates KDTree object and query it for indexes of points in FESOM mesh that are close to the\n",
    "    points of the target grid. Also return distances of the original points to target points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mesh : fesom_mesh object\n",
    "        pyfesom mesh representation\n",
    "    lons/lats : array\n",
    "        2d arrays with target grid values.\n",
    "    k : int\n",
    "        k-th nearest neighbors to return.\n",
    "    n_jobs : int, optional\n",
    "        Number of jobs to schedule for parallel processing. If -1 is given\n",
    "        all processors are used. Default: 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distances : array of floats\n",
    "        The distances to the nearest neighbors.\n",
    "    inds : ndarray of ints\n",
    "        The locations of the neighbors in data.\n",
    "\n",
    "    \"\"\"\n",
    "    xs, ys, zs = lon_lat_to_cartesian(mesh.x2, mesh.y2)\n",
    "    xt, yt, zt = lon_lat_to_cartesian(lons.flatten(), lats.flatten())\n",
    "\n",
    "    tree = cKDTree(list(zip(xs, ys, zs)))\n",
    "    distances, inds = tree.query(list(zip(xt, yt, zt)), k=k, n_jobs=n_jobs)\n",
    "\n",
    "    return distances, inds\n",
    "\n",
    "def fesom2regular(\n",
    "    data,\n",
    "    mesh,\n",
    "    lons,\n",
    "    lats,\n",
    "    distances_path=None,\n",
    "    inds_path=None,\n",
    "    qhull_path=None,\n",
    "    how=\"nn\",\n",
    "    k=5,\n",
    "    radius_of_influence=100000,\n",
    "    n_jobs=2,\n",
    "    dumpfile=True,\n",
    "    basepath=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Interpolates data from FESOM mesh to target (usually regular) mesh.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array\n",
    "        1d array that represents FESOM data at one\n",
    "    mesh : fesom_mesh object\n",
    "        pyfesom mesh representation\n",
    "    lons/lats : array\n",
    "        2d arrays with target grid values.\n",
    "    distances_path : string\n",
    "        Path to the file with distances. If not provided and dumpfile=True, it will be created.\n",
    "    inds_path : string\n",
    "        Path to the file with inds. If not provided and dumpfile=True, it will be created.\n",
    "    qhull_path : str\n",
    "         Path to the file with qhull (needed for linear and cubic interpolations). If not provided and dumpfile=True, it will be created.\n",
    "    how : str\n",
    "       Interpolation method. Options are 'nn' (nearest neighbor), 'idist' (inverce distance), \"linear\" and \"cubic\".\n",
    "    k : int\n",
    "        k-th nearest neighbors to use. Only used when how==idist\n",
    "    radius_of_influence : int\n",
    "        Cut off distance in meters, only used in nn and idist.\n",
    "    n_jobs : int, optional\n",
    "        Number of jobs to schedule for parallel processing. If -1 is given\n",
    "        all processors are used. Default: 1. Only used for nn and idist.\n",
    "    dumpfile: bool\n",
    "        wether to dump resulted distances and inds to the file.\n",
    "    basepath: str\n",
    "        path where to store additional interpolation files. If None (default),\n",
    "        the path of the mesh will be used.\n",
    "    Returns\n",
    "    -------\n",
    "    data_interpolated : 2d array\n",
    "        array with data interpolated to the target grid.\n",
    "    \"\"\"\n",
    "\n",
    "    left, right, down, up = np.min(lons), np.max(lons), np.min(lats), np.max(lats)\n",
    "    lonNumber, latNumber = lons.shape[1], lats.shape[0]\n",
    "\n",
    "    if how == \"nn\":\n",
    "        kk = 1\n",
    "    else:\n",
    "        kk = k\n",
    "\n",
    "    distances_paths = []\n",
    "    inds_paths = []\n",
    "    qhull_paths = []\n",
    "\n",
    "    MESH_BASE = os.path.basename(mesh.path)\n",
    "    MESH_DIR = mesh.path\n",
    "    CACHE_DIR = os.environ.get(\"PYFESOM_CACHE\", os.path.join(os.getcwd(), \"MESH_cache\"))\n",
    "    CACHE_DIR = os.path.join(CACHE_DIR, MESH_BASE)\n",
    "\n",
    "    if not os.path.isdir(CACHE_DIR):\n",
    "        os.makedirs(CACHE_DIR)\n",
    "\n",
    "    distances_file = \"distances_{}_{}_{}_{}_{}_{}_{}_{}\".format(\n",
    "        mesh.n2d, left, right, down, up, lonNumber, latNumber, kk\n",
    "    )\n",
    "    inds_file = \"inds_{}_{}_{}_{}_{}_{}_{}_{}\".format(\n",
    "        mesh.n2d, left, right, down, up, lonNumber, latNumber, kk\n",
    "    )\n",
    "    qhull_file = \"qhull_{}\".format(mesh.n2d)\n",
    "\n",
    "    distances_paths.append(os.path.join(mesh.path, distances_file))\n",
    "    distances_paths.append(os.path.join(CACHE_DIR, distances_file))\n",
    "\n",
    "    inds_paths.append(os.path.join(mesh.path, inds_file))\n",
    "    inds_paths.append(os.path.join(CACHE_DIR, inds_file))\n",
    "\n",
    "    qhull_paths.append(os.path.join(mesh.path, qhull_file))\n",
    "    qhull_paths.append(os.path.join(CACHE_DIR, qhull_file))\n",
    "\n",
    "    # if distances_path is provided, use it first\n",
    "    if distances_path is not None:\n",
    "        distances_paths.insert(0, distances_path)\n",
    "\n",
    "    if inds_path is not None:\n",
    "        inds_paths.insert(0, inds_path)\n",
    "\n",
    "    if qhull_path is not None:\n",
    "        qhull_paths.insert(0, qhull_path)\n",
    "\n",
    "    loaded_distances = False\n",
    "    loaded_inds = False\n",
    "    loaded_qhull = False\n",
    "    if how == \"nn\":\n",
    "        for distances_path in distances_paths:\n",
    "            if os.path.isfile(distances_path):\n",
    "                logging.info(\n",
    "                    \"Note: using precalculated file from {}\".format(distances_path)\n",
    "                )\n",
    "                try:\n",
    "                    distances = joblib.load(distances_path)\n",
    "                    loaded_distances = True\n",
    "                    break\n",
    "                except PermissionError:\n",
    "                    # who knows, something didn't work. Try the next path:\n",
    "                    continue\n",
    "        for inds_path in inds_paths:\n",
    "            if os.path.isfile(inds_path):\n",
    "                logging.info(\"Note: using precalculated file from {}\".format(inds_path))\n",
    "                try:\n",
    "                    inds = joblib.load(inds_path)\n",
    "                    loaded_inds = True\n",
    "                    break\n",
    "                except PermissionError:\n",
    "                    # Same as above...something is wrong\n",
    "                    continue\n",
    "        if not (loaded_distances and loaded_inds):\n",
    "            distances, inds = create_indexes_and_distances(\n",
    "                mesh, lons, lats, k=kk, n_jobs=n_jobs\n",
    "            )\n",
    "            if dumpfile:\n",
    "                for distances_path in distances_paths:\n",
    "                    try:\n",
    "                        joblib.dump(distances, distances_path)\n",
    "                        break\n",
    "                    except PermissionError:\n",
    "                        # Couldn't dump the file, try next path\n",
    "                        continue\n",
    "                for inds_path in inds_paths:\n",
    "                    try:\n",
    "                        joblib.dump(inds, inds_path)\n",
    "                        break\n",
    "                    except PermissionError:\n",
    "                        # Couldn't dump inds file, try next\n",
    "                        continue\n",
    "\n",
    "        data_interpolated = data[inds]\n",
    "        data_interpolated[distances >= radius_of_influence] = np.nan\n",
    "        data_interpolated = data_interpolated.reshape(lons.shape)\n",
    "        data_interpolated = np.ma.masked_invalid(data_interpolated)\n",
    "        return data_interpolated\n",
    "\n",
    "    elif how == \"idist\":\n",
    "        for distances_path in distances_paths:\n",
    "            if os.path.isfile(distances_path):\n",
    "                logging.info(\n",
    "                    \"Note: using precalculated file from {}\".format(distances_path)\n",
    "                )\n",
    "                try:\n",
    "                    distances = joblib.load(distances_path)\n",
    "                    loaded_distances = True\n",
    "                    break\n",
    "                except PermissionError:\n",
    "                    # who knows, something didn't work. Try the next path:\n",
    "                    continue\n",
    "        for inds_path in inds_paths:\n",
    "            if os.path.isfile(inds_path):\n",
    "                logging.info(\"Note: using precalculated file from {}\".format(inds_path))\n",
    "                try:\n",
    "                    inds = joblib.load(inds_path)\n",
    "                    loaded_inds = True\n",
    "                    break\n",
    "                except PermissionError:\n",
    "                    # Same as above...something is wrong\n",
    "                    continue\n",
    "        if not (loaded_distances and loaded_inds):\n",
    "            distances, inds = create_indexes_and_distances(\n",
    "                mesh, lons, lats, k=kk, n_jobs=n_jobs\n",
    "            )\n",
    "            if dumpfile:\n",
    "                for distances_path in distances_paths:\n",
    "                    try:\n",
    "                        joblib.dump(distances, distances_path)\n",
    "                        break\n",
    "                    except PermissionError:\n",
    "                        # Couldn't dump the file, try next path\n",
    "                        continue\n",
    "                for inds_path in inds_paths:\n",
    "                    try:\n",
    "                        joblib.dump(inds, inds_path)\n",
    "                        break\n",
    "                    except PermissionError:\n",
    "                        # Couldn't dump inds file, try next\n",
    "                        continue\n",
    "\n",
    "        distances_ma = np.ma.masked_greater(distances, radius_of_influence)\n",
    "\n",
    "        w = 1.0 / distances_ma ** 2\n",
    "        data_interpolated = np.ma.sum(w * data[inds], axis=1) / np.ma.sum(w, axis=1)\n",
    "        data_interpolated.shape = lons.shape\n",
    "        data_interpolated = np.ma.masked_invalid(data_interpolated)\n",
    "        return data_interpolated\n",
    "\n",
    "    elif how == \"linear\":\n",
    "        for qhull_path in qhull_paths:\n",
    "            if os.path.isfile(qhull_path):\n",
    "                logging.info(\n",
    "                    \"Note: using precalculated file from {}\".format(qhull_path)\n",
    "                )\n",
    "                try:\n",
    "                    qh = joblib.load(qhull_path)\n",
    "                    loaded_qhull = True\n",
    "                    break\n",
    "                except PermissionError:\n",
    "                    # who knows, something didn't work. Try the next path:\n",
    "                    continue\n",
    "        if not loaded_qhull:\n",
    "            points = np.vstack((mesh.x2, mesh.y2)).T\n",
    "            qh = qhull.Delaunay(points)\n",
    "            if dumpfile:\n",
    "                for qhull_path in qhull_paths:\n",
    "                    try:\n",
    "                        joblib.dump(qh, qhull_path)\n",
    "                        break\n",
    "                    except PermissionError:\n",
    "                        continue\n",
    "        data_interpolated = LinearNDInterpolator(qh, data)((lons, lats))\n",
    "        data_interpolated = np.ma.masked_invalid(data_interpolated)\n",
    "        return data_interpolated\n",
    "\n",
    "    elif how == \"cubic\":\n",
    "        for qhull_path in qhull_paths:\n",
    "            if os.path.isfile(qhull_path):\n",
    "                logging.info(\n",
    "                    \"Note: using precalculated file from {}\".format(qhull_path)\n",
    "                )\n",
    "                logging.info(\n",
    "                    \"Note: using precalculated file from {}\".format(qhull_path)\n",
    "                )\n",
    "                try:\n",
    "                    qh = joblib.load(qhull_path)\n",
    "                    loaded_qhull = True\n",
    "                    break\n",
    "                except PermissionError:\n",
    "                    # who knows, something didn't work. Try the next path:\n",
    "                    continue\n",
    "        if not loaded_qhull:\n",
    "            points = np.vstack((mesh.x2, mesh.y2)).T\n",
    "            qh = qhull.Delaunay(points)\n",
    "            if dumpfile:\n",
    "                for qhull_path in qhull_paths:\n",
    "                    try:\n",
    "                        joblib.dump(qh, qhull_path)\n",
    "                        break\n",
    "                    except PermissionError:\n",
    "                        continue\n",
    "        data_interpolated = CloughTocher2DInterpolator(qh, data)((lons, lats))\n",
    "        data_interpolated = np.ma.masked_invalid(data_interpolated)\n",
    "        return data_interpolated\n",
    "    else:\n",
    "        raise ValueError(\"Interpolation method is not supported\")\n",
    "\n",
    "def mask_ne(lonreg2, latreg2):\n",
    "    \"\"\" Mask earth from lon/lat data using Natural Earth.\n",
    "    Parameters\n",
    "    ----------\n",
    "    lonreg2: float, np.array\n",
    "        2D array of longitudes\n",
    "    latreg2: float, np.array\n",
    "        2D array of latitudes\n",
    "    Returns\n",
    "    -------\n",
    "    m2: bool, np.array\n",
    "        2D mask with True where the ocean is.\n",
    "    \"\"\"\n",
    "    nearth = cfeature.NaturalEarthFeature(\"physical\", \"ocean\", \"50m\")\n",
    "    main_geom = [contour for contour in nearth.geometries()][0]\n",
    "\n",
    "    mask = shapely.vectorized.contains(main_geom, lonreg2, latreg2)\n",
    "    m2 = np.where(((lonreg2 == -180.0) & (latreg2 > 71.5)), True, mask)\n",
    "    m2 = np.where(\n",
    "        ((lonreg2 == -180.0) & (latreg2 < 70.95) & (latreg2 > 68.96)), True, m2\n",
    "    )\n",
    "    m2 = np.where(((lonreg2 == 180.0) & (latreg2 > 71.5)), True, mask)\n",
    "    m2 = np.where(\n",
    "        ((lonreg2 == 180.0) & (latreg2 < 70.95) & (latreg2 > 68.96)), True, m2\n",
    "    )\n",
    "    # m2 = np.where(\n",
    "    #        ((lonreg2 == 180.0) & (latreg2 > -75.0) & (latreg2 < 0)), True, m2\n",
    "    #    )\n",
    "    m2 = np.where(((lonreg2 == -180.0) & (latreg2 < 65.33)), True, m2)\n",
    "    m2 = np.where(((lonreg2 == 180.0) & (latreg2 < 65.33)), True, m2)\n",
    "\n",
    "    return ~m2\n",
    "\n",
    "def lon_lat_to_cartesian(lon, lat, R=6371000):\n",
    "    \"\"\"\n",
    "    calculates lon, lat coordinates of a point on a sphere with\n",
    "    radius R. Taken from http://earthpy.org/interpolation_between_grids_with_ckdtree.html\n",
    "    \"\"\"\n",
    "    lon_r = np.radians(lon)\n",
    "    lat_r = np.radians(lat)\n",
    "\n",
    "    x = R * np.cos(lat_r) * np.cos(lon_r)\n",
    "    y = R * np.cos(lat_r) * np.sin(lon_r)\n",
    "    z = R * np.sin(lat_r)\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "def create_proj_figure(mapproj, rowscol, figsize):\n",
    "    \"\"\" Create figure and axis with cartopy projection.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mapproj: str\n",
    "        name of the projection:\n",
    "            merc: Mercator\n",
    "            pc: PlateCarree (default)\n",
    "            np: NorthPolarStereo\n",
    "            sp: SouthPolarStereo\n",
    "            rob: Robinson\n",
    "    rowcol: (int, int)\n",
    "        number of rows and columns of the figure.\n",
    "    figsize: (float, float)\n",
    "        width, height in inches.\n",
    "    Returns\n",
    "    -------\n",
    "    fig, ax\n",
    "    \"\"\"\n",
    "    if mapproj == \"merc\":\n",
    "        fig, ax = plt.subplots(\n",
    "            rowscol[0],\n",
    "            rowscol[1],\n",
    "            subplot_kw=dict(projection=ccrs.Mercator()),\n",
    "            constrained_layout=True,\n",
    "            figsize=figsize,\n",
    "        )\n",
    "    elif mapproj == \"pc\":\n",
    "        fig, ax = plt.subplots(\n",
    "            rowscol[0],\n",
    "            rowscol[1],\n",
    "            subplot_kw=dict(projection=ccrs.PlateCarree()),\n",
    "            constrained_layout=True,\n",
    "            figsize=figsize,\n",
    "        )\n",
    "    elif mapproj == \"np\":\n",
    "        fig, ax = plt.subplots(\n",
    "            rowscol[0],\n",
    "            rowscol[1],\n",
    "            subplot_kw=dict(projection=ccrs.NorthPolarStereo()),\n",
    "            constrained_layout=True,\n",
    "            figsize=figsize,\n",
    "        )\n",
    "    elif mapproj == \"sp\":\n",
    "        fig, ax = plt.subplots(\n",
    "            rowscol[0],\n",
    "            rowscol[1],\n",
    "            subplot_kw=dict(projection=ccrs.SouthPolarStereo()),\n",
    "            constrained_layout=True,\n",
    "            figsize=figsize,\n",
    "        )\n",
    "    elif mapproj == \"rob\":\n",
    "        fig, ax = plt.subplots(\n",
    "            rowscol[0],\n",
    "            rowscol[1],\n",
    "            subplot_kw=dict(projection=ccrs.Robinson()),\n",
    "            constrained_layout=True,\n",
    "            figsize=figsize,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Projection {mapproj} is not supported.\")\n",
    "    return fig, ax\n",
    "\n",
    "def get_plot_levels(levels, data, lev_to_data=False):\n",
    "    \"\"\"Returns levels for the plot.\n",
    "    Parameters\n",
    "    ----------\n",
    "    levels: list, numpy array\n",
    "        Can be list or numpy array with three or more elements.\n",
    "        If only three elements provided, they will b einterpereted as min, max, number of levels.\n",
    "        If more elements provided, they will be used directly.\n",
    "    data: numpy array of xarray\n",
    "        Data, that should be plotted with this levels.\n",
    "    lev_to_data: bool\n",
    "        Switch to correct the levels to the actual data range.\n",
    "        This is needed for safe plotting on triangular grid with cartopy.\n",
    "    Returns\n",
    "    -------\n",
    "    data_levels: numpy array\n",
    "        resulted levels.\n",
    "    \"\"\"\n",
    "    if levels is not None:\n",
    "        if len(levels) == 3:\n",
    "            mmin, mmax, nnum = levels\n",
    "            if lev_to_data:\n",
    "                mmin, mmax = levels_to_data(mmin, mmax, data)\n",
    "            nnum = int(nnum)\n",
    "            data_levels = np.linspace(mmin, mmax, nnum)\n",
    "        elif len(levels) < 3:\n",
    "            raise ValueError(\n",
    "                \"Levels can be the list or numpy array with three or more elements.\"\n",
    "            )\n",
    "        else:\n",
    "            data_levels = np.array(levels)\n",
    "    else:\n",
    "        mmin = np.nanmin(data)\n",
    "        mmax = np.nanmax(data)\n",
    "        nnum = 40\n",
    "        data_levels = np.linspace(mmin, mmax, nnum)\n",
    "    return data_levels\n",
    "\n",
    "\n",
    "def plot(\n",
    "    mesh,\n",
    "    data,\n",
    "    cmap=None,\n",
    "    influence=80000,\n",
    "    box=[-180, 180, -89, 90],\n",
    "    res=[360, 180],\n",
    "    interp=\"nn\",\n",
    "    mapproj=\"pc\",\n",
    "    levels=None,\n",
    "    ptype=\"cf\",\n",
    "    units=None,\n",
    "    figsize=(6, 4.5),\n",
    "    rowscol=(1, 1),\n",
    "    titles=None,\n",
    "    distances_path=None,\n",
    "    inds_path=None,\n",
    "    qhull_path=None,\n",
    "    basepath=None,\n",
    "    interpolated_data=None,\n",
    "    lonreg=None,\n",
    "    latreg=None,\n",
    "    no_pi_mask=False,\n",
    "    rmsdval=None,\n",
    "    mdval=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots interpolated 2d field on the map.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mesh: mesh object\n",
    "        FESOM2 mesh object\n",
    "    data: np.array or list of np.arrays\n",
    "        FESOM 2 data on nodes (for u,v,u_ice and v_ice one have to first interpolate from elements to nodes).\n",
    "        Can be ether one np.ndarray or list of np.ndarrays.\n",
    "    cmap: str\n",
    "        Name of the colormap from cmocean package or from the standard matplotlib set.\n",
    "        By default `Spectral_r` will be used.\n",
    "    influence: float\n",
    "        Radius of influence for interpolation, in meters.\n",
    "    box: list\n",
    "        Map boundaries in -180 180 -90 90 format that will be used for interpolation (default [-180 180 -89 90]).\n",
    "    res: list\n",
    "        Number of points along each axis that will be used for interpolation (for lon and lat),\n",
    "        default [360, 180].\n",
    "    interp: str\n",
    "        Interpolation method. Options are 'nn' (nearest neighbor), 'idist' (inverce distance), \"linear\" and \"cubic\".\n",
    "    mapproj: str\n",
    "        Map projection. Options are Mercator (merc), Plate Carree (pc),\n",
    "        North Polar Stereo (np), South Polar Stereo (sp),  Robinson (rob)\n",
    "    levels: list\n",
    "        Levels for contour plot in format (min, max, numberOfLevels). List with more than\n",
    "        3 values will be interpreted as just a list of individual level values.\n",
    "        If not provided min/max values from data will be used with 40 levels.\n",
    "    ptype: str\n",
    "        Plot type. Options are contourf (\\'cf\\') and pcolormesh (\\'pcm\\')\n",
    "    units: str\n",
    "        Units for color bar.\n",
    "    figsize: tuple\n",
    "        figure size in inches\n",
    "    rowscol: tuple\n",
    "        number of rows and columns.\n",
    "    titles: str or list\n",
    "        Title of the plot (if string) or subplots (if list of strings)\n",
    "    distances_path : string\n",
    "        Path to the file with distances. If not provided and dumpfile=True, it will be created.\n",
    "    inds_path : string\n",
    "        Path to the file with inds. If not provided and dumpfile=True, it will be created.\n",
    "    qhull_path : str\n",
    "         Path to the file with qhull (needed for linear and cubic interpolations).\n",
    "         If not provided and dumpfile=True, it will be created.\n",
    "    interpolated_data: np.array\n",
    "         data interpolated to regular grid (you also have to provide lonreg and latreg).\n",
    "         If provided, data will be plotted directly, without interpolation.\n",
    "    lonreg: np.array\n",
    "         1D array of longitudes. Used in combination with `interpolated_data`,\n",
    "         when you need to plot interpolated data directly.\n",
    "    latreg: np.array\n",
    "         1D array of latitudes. Used in combination with `interpolated_data`,\n",
    "         when you need to plot interpolated data directly.\n",
    "    basepath: str\n",
    "        path where to store additional interpolation files. If None (default),\n",
    "        the path of the mesh will be used.\n",
    "    no_pi_mask: bool\n",
    "        Mask PI by default or not.\n",
    "    \"\"\"\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "    if titles:\n",
    "        if not isinstance(titles, list):\n",
    "            titles = [titles]\n",
    "        if len(titles) != len(data):\n",
    "            raise ValueError(\n",
    "                \"The number of titles do not match the number of data fields, please adjust titles (or put to None)\"\n",
    "            )\n",
    "\n",
    "    if (rowscol[0] * rowscol[1]) < len(data):\n",
    "        raise ValueError(\n",
    "            \"Number of rows*columns is smaller than number of data fields, please adjust rowscol.\"\n",
    "        )\n",
    "\n",
    "    colormap = get_cmap(cmap=cmap)\n",
    "\n",
    "    radius_of_influence = influence\n",
    "\n",
    "    left, right, down, up = box\n",
    "    lonNumber, latNumber = res\n",
    "\n",
    "    if lonreg is None:\n",
    "        lonreg = np.linspace(left, right, lonNumber)\n",
    "        latreg = np.linspace(down, up, latNumber)\n",
    "\n",
    "    lonreg2, latreg2 = np.meshgrid(lonreg, latreg)\n",
    "\n",
    "    if interpolated_data is None:\n",
    "        interpolated = interpolate_for_plot(\n",
    "            data,\n",
    "            mesh,\n",
    "            lonreg2,\n",
    "            latreg2,\n",
    "            interp=interp,\n",
    "            distances_path=distances_path,\n",
    "            inds_path=inds_path,\n",
    "            radius_of_influence=radius_of_influence,\n",
    "            basepath=basepath,\n",
    "            qhull_path=qhull_path,\n",
    "        )\n",
    "    else:\n",
    "        interpolated = [interpolated_data]\n",
    "\n",
    "    m2 = mask_ne(lonreg2, latreg2)\n",
    "\n",
    "    for i in range(len(interpolated)):\n",
    "        if not no_pi_mask:\n",
    "            interpolated[i] = np.ma.masked_where(m2, interpolated[i])\n",
    "        interpolated[i] = np.ma.masked_equal(interpolated[i], 0)\n",
    "\n",
    "    fig, ax = create_proj_figure(mapproj, rowscol, figsize)\n",
    "\n",
    "    if isinstance(ax, np.ndarray):\n",
    "        ax = ax.flatten()\n",
    "    else:\n",
    "        ax = [ax]\n",
    "\n",
    "    for ind, data_int in enumerate(interpolated):\n",
    "        ax[ind].set_extent([left, right, down, up], crs=ccrs.PlateCarree())\n",
    "\n",
    "        data_levels = get_plot_levels(levels, data_int, lev_to_data=False)\n",
    "\n",
    "        if ptype == \"cf\":\n",
    "            data_int_cyc, lon_cyc = add_cyclic_point(data_int, coord=lonreg)\n",
    "            image = ax[ind].contourf(\n",
    "                lon_cyc,\n",
    "                latreg,\n",
    "                data_int_cyc,\n",
    "                levels=data_levels,\n",
    "                transform=ccrs.PlateCarree(),\n",
    "                cmap=colormap,\n",
    "                extend=\"both\",\n",
    "            )\n",
    "        elif ptype == \"pcm\":\n",
    "            mmin = data_levels[0]\n",
    "            mmax = data_levels[-1]\n",
    "            data_int_cyc, lon_cyc = add_cyclic_point(data_int, coord=lonreg)\n",
    "            image = ax[ind].pcolormesh(\n",
    "                lon_cyc,\n",
    "                latreg,\n",
    "                data_int_cyc,\n",
    "                vmin=mmin,\n",
    "                vmax=mmax,\n",
    "                transform=ccrs.PlateCarree(),\n",
    "                cmap=colormap,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Inknown plot type {}\".format(ptype))\n",
    "\n",
    "        # ax.coastlines(resolution = '50m',lw=0.5)\n",
    "        ax[ind].add_feature(\n",
    "            cfeature.GSHHSFeature(levels=[1], scale=\"low\", facecolor=\"lightgray\")\n",
    "        )\n",
    "        if titles:\n",
    "            titles = titles.copy()\n",
    "            ax[ind].set_title(titles.pop(0), fontweight='bold')\n",
    "\n",
    "    for delind in range(ind + 1, len(ax)):\n",
    "        fig.delaxes(ax[delind])\n",
    "\n",
    "\n",
    "    gl = ax[ind].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                  linewidth=1, color='gray', alpha=0.2, linestyle='-')\n",
    "\n",
    "    gl.xlabels_bottom = False\n",
    "        \n",
    "    textrsmd='rmsd='+str(round(rmsdval,3))\n",
    "    textbias='bias='+str(round(mdval,3))\n",
    "    props = dict(boxstyle='round,pad=0.1', facecolor='white', alpha=0.5)\n",
    "    ax[ind].text(0.02, 0.4, textrsmd, transform=ax[ind].transAxes, fontsize=13,\n",
    "        verticalalignment='top', bbox=props, zorder=4)\n",
    "    ax[ind].text(0.02, 0.3, textbias, transform=ax[ind].transAxes, fontsize=13,\n",
    "        verticalalignment='top', bbox=props, zorder=4)\n",
    "\n",
    "    cbar_ax_abs = fig.add_axes([0.15, 0.10, 0.7, 0.05])\n",
    "    cbar_ax_abs.tick_params(labelsize=12)\n",
    "    cb = fig.colorbar(image, cax=cbar_ax_abs, orientation='horizontal',ticks=levels)\n",
    "    cb.set_label(label=units, size='14')\n",
    "    cb.ax.tick_params(labelsize='12')\n",
    "    for label in cb.ax.xaxis.get_ticklabels()[::2]:\n",
    "        label.set_visible(False)\n",
    "    return ax,latreg\n",
    "\n",
    "\n",
    "title2 = \"SST_bias_\"\n",
    "ofile = title2 + str(depth)\n",
    "print(ofile)\n",
    "if input_names is None:\n",
    "    input_names = []\n",
    "    for run in input_paths:\n",
    "        run = os.path.join(run, '')\n",
    "        input_names.append(run.split('/')[-2])\n",
    "\n",
    "mesh = pf.load_mesh(meshpath, abg=abg, \n",
    "                    usepickle=True, usejoblib=False)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(vars(mesh))\n",
    "\n",
    "plotds = OrderedDict()\n",
    "data_reference = pf.get_data(reference_path, variable, reference_years, mesh, depth=depth, how=how, compute=True, silent=True)\n",
    "plotds[depth] = {}\n",
    "\n",
    "for exp_path, exp_name in zip(input_paths, input_names):\n",
    "    data_test = pf.get_data(exp_path, variable, years, mesh, depth=depth, how=how, compute=True, silent=True)\n",
    "    data_difference = data_test - data_reference\n",
    "    title = exp_name + \" - \" + reference_name\n",
    "    plotds[depth][title] = {}\n",
    "    plotds[depth][title]['data'] = data_difference\n",
    "    if (data_difference.max() == data_difference.min() == 0):\n",
    "        plotds[depth][title]['nodiff'] = True\n",
    "    else:\n",
    "        plotds[depth][title]['nodiff'] = False\n",
    "\n",
    "mesh_data = Dataset(meshpath + '/' + mesh_file)\n",
    "wgts = np.array(mesh_data['cell_area'][:]).flatten()\n",
    "wgts = np.broadcast_to(wgts, data_test.shape)\n",
    "\n",
    "rmsdval = sqrt(mean_squared_error(data_test, data_reference, sample_weight=np.array(wgts)))\n",
    "mdval = md(data_test, data_reference, wgts)\n",
    "\n",
    "sfmt = ticker.ScalarFormatter(useMathText=True)\n",
    "sfmt.set_powerlimits((-3, 4))\n",
    "\n",
    "levels = [-5.0, -3.0, -2.0, -1.0, -0.6, -0.2, 0.2, 0.6, 1.0, 2.0, 3.0, 5.0]\n",
    "\n",
    "figsize = (6, 4.5)\n",
    "dpi = 300\n",
    "\n",
    "plot_data, plot_names = data_to_plot(plotds, depth)\n",
    "if not plot_data:\n",
    "    print('There is no difference between fields')\n",
    "    identical = True\n",
    "else:\n",
    "    identical = False\n",
    "\n",
    "if len(plot_data) == 1:\n",
    "    plot_data = plot_data[0]\n",
    "    plot_names = plot_names[0]\n",
    "\n",
    "if not identical:\n",
    "    plot(mesh, \n",
    "         plot_data,\n",
    "         rowscol=rowscol,\n",
    "         mapproj=mapproj,\n",
    "         cmap='PuOr_r',\n",
    "         levels=levels,\n",
    "         figsize=figsize,\n",
    "         box=bbox,\n",
    "         res=res,\n",
    "         units=units,\n",
    "         titles=ofile,\n",
    "         rmsdval=rmsdval,\n",
    "         mdval=mdval)\n",
    "\n",
    "if ofile is not None:\n",
    "    full_path = out_path + \"/\" + tripyrun_name + '_part13_fesom_bias_maps_' + ofile + '.png'\n",
    "    plt.savefig(full_path, dpi=dpi, bbox_inches='tight')\n",
    "    saved_filenames.append(full_path)\n",
    "    print(f\"DEBUG: Bild gespeichert als: {full_path}\")\n",
    "\n",
    "update_status(SCRIPT_NAME, \"Completed\")\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "for depth in [0,100,1000,4000]:\n",
    "    if depths==0:\n",
    "        title2 = \"SST_bias_\"\n",
    "        ofile=title2+str(depth)\n",
    "    elif depth==100:\n",
    "        title2 = \"SST_bias_\"\n",
    "        ofile=title2+str(depth)\n",
    "    elif depth==1000:\n",
    "        title2 =\"SST_bias_\"\n",
    "        ofile=title2+str(depth)\n",
    "    elif depth==4000:\n",
    "        title2 = \"SST_bias_\"\n",
    "        ofile=title2+str(depth)\n",
    "\n",
    "    if input_names is None:\n",
    "        input_names = []\n",
    "        for run in input_paths:\n",
    "            run = os.path.join(run, '')\n",
    "            input_names.append(run.split('/')[-2])\n",
    "\n",
    "    mesh = pf.load_mesh(meshpath, abg=abg, \n",
    "                        usepickle=True, usejoblib=False)\n",
    "\n",
    "    from pprint import pprint\n",
    "    pprint(vars(mesh))\n",
    "\n",
    "    plotds = OrderedDict()\n",
    "    data_reference = pf.get_data(reference_path, variable, reference_years, mesh, depth = depth, how=how, compute=True, silent=True)\n",
    "    plotds[depth] = {}\n",
    "    for exp_path, exp_name  in zip(input_paths, input_names):\n",
    "        data_test      = pf.get_data(exp_path, variable, years, mesh, depth = depth, how=how, compute=True, silent=True)\n",
    "        data_difference= data_test - data_reference\n",
    "        title = exp_name+\" - \"+reference_name\n",
    "        plotds[depth][title] = {}\n",
    "        plotds[depth][title]['data'] = data_difference\n",
    "        if (data_difference.max() == data_difference.min() == 0):\n",
    "            plotds[depth][title]['nodiff'] = True\n",
    "        else:\n",
    "            plotds[depth][title]['nodiff'] = False\n",
    "\n",
    "    mesh_data = Dataset(meshpath+'/'+mesh_file)\n",
    "    wgts = np.array(mesh_data['cell_area'][:]).flatten()  # Ensure it's 1D\n",
    "    wgts = np.broadcast_to(wgts, data_test.shape)  # Expand to match data shape\n",
    "\n",
    "    wgts=mesh_data['cell_area']\n",
    "    rmsdval = sqrt(mean_squared_error(data_test, data_reference, sample_weight=np.array(wgts)))\n",
    "    mdval = md(data_test, data_reference, wgts)\n",
    "\n",
    "    sfmt = ticker.ScalarFormatter(useMathText=True)\n",
    "    sfmt.set_powerlimits((-3, 4))\n",
    "\n",
    "    levels = [-5.0,-3.0,-2.0,-1.0,-.6,-.2,.2,.6,1.0,2.0,3.0,5.0]\n",
    "\n",
    "    figsize=(6, 4.5)\n",
    "    dpi=300\n",
    "\n",
    "    plot_data, plot_names = data_to_plot(plotds, depth)\n",
    "    if not plot_data:\n",
    "        print('There is no difference between fields')\n",
    "        identical = True\n",
    "    else:\n",
    "        identical = False\n",
    "\n",
    "    if len(plot_data) == 1:\n",
    "        plot_data = plot_data[0]\n",
    "        plot_names = plot_names[0]\n",
    "\n",
    "\n",
    "    if not identical:\n",
    "        plot(mesh, \n",
    "            plot_data,\n",
    "            rowscol=rowscol,\n",
    "            mapproj=mapproj,\n",
    "            cmap='PuOr_r', \n",
    "            levels=levels,\n",
    "            figsize = figsize, \n",
    "            box=bbox, \n",
    "            res = res,\n",
    "            units = units,\n",
    "            titles = title2,\n",
    "            rmsdval = rmsdval,\n",
    "            mdval = mdval);\n",
    "\n",
    "\n",
    "    if ofile is not None:\n",
    "        plt.savefig(out_path+\"/\"+tripyrun_name +'_part13_fesom_bias_maps_'+ofile, dpi=dpi, bbox_inches='tight')\n",
    "        saved_filenames.append(out_path+ofile+'.png')\n",
    "\n",
    "# Mark script as completed\n",
    "update_status(SCRIPT_NAME, \"Completed\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77540aad-1b83-4151-ac94-75533607c790",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reval)",
   "language": "python",
   "name": "reval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
