{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tripyview.sub_notebookheader as nb_setup\n",
    "# which_matplotlib = 'inline' (default), \n",
    "#                    'notebook'(jupyter notebook), \n",
    "#                    'widget'(jupyterlab)\n",
    "nb_setup.init_notebook(which_matplotlib=\"inline\")\n",
    "# centralized autoimport of: \n",
    "# import os\n",
    "# import warnings\n",
    "# import time as clock\n",
    "# import numpy as np\n",
    "# import xarray as xr\n",
    "# import shapefile as shp\n",
    "# import tripyview as tpv\n",
    "# client, use_existing_client = None, \"tcp://0.0.0.0:0000\"\n",
    "# use_existing_client='tcp://127.0.0.1:44299'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFO:\n",
    "To compute especially on large unstructured grids #vertices>1M, you need to run this notebook in parallel (do_parallel=True) on several workers (parallel_nprc...is the number of dask worker that can be allocated, parallel_tmem...is the maximum available RAM that will be distributed between the dask workers). Therefor allocate a full !!! COMPUTE NODE !!! (admins might not be happy if you do this in parallel on a login node) of a HPC of your choice with as much memory (RAM) as you can get to run this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "#___Dask Client Parameters____________________________________________________________\n",
    "do_papermill      = False\n",
    "do_parallel       = True\n",
    "parallel_nprc     = 64   # number of total processes\n",
    "parallel_nthread  = 2    # number of threads per worker --> number worker = parallel_nprc/parallel_nthread\n",
    "parallel_tmem     = 256  # max. available RAM\n",
    "\n",
    "#___Mesh Path & Save Path_____________________________________________________________\n",
    "# mesh_path ='/work/ollie/projects/clidyn/FESOM2/meshes/core2/'\n",
    "# mesh_path         = '/albedo/work/user/pscholz/mesh_fesom2.0/core2_srt_dep@node/'\n",
    "mesh_path         = '/albedo/work/user/pscholz/mesh_fesom2.0/dart/'\n",
    "# mesh_path         = '/work/ba1264/a270210/model/input/fesom2/dart/'\n",
    "\n",
    "save_path         = None #'~/figures/test_papermill/'\n",
    "save_fname        = None # filename from papermill come in through save_fname\n",
    "tripyrun_name     = None # papermill workflow name of notebook \n",
    "tripyrun_analysis = None # papermill diagnostic driver\n",
    "tripyrun_spath_nb = None # papermill path to processed notebooks\n",
    "tripyrun_spath_fig= None # papermill path to processed figures\n",
    "\n",
    "#___Data Path & Input Names___________________________________________________________\n",
    "input_paths   = list()\n",
    "# input_paths.append('/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke+idemix_jayne_bin_ck0.1/5/')\n",
    "# input_paths.append('/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke+idemix_nycander_bin_ck0.1/5/')\n",
    "# input_paths.append('/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke+idemix_stormtide_bin_ck0.1/5/')\n",
    "# input_paths.append('/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke_ctrl_ck0.1/5/')\n",
    "# input_paths.append('/work/ab0995/a270210/runtime/awicm3-v3.1_refactoring/TCO319L137-DART/hist1950d/outdata/fesom/')\n",
    "input_paths.append('/albedo/work/user/pscholz/results/dart_linfs_pc0_ctrl_1/1/')\n",
    "\n",
    "input_names   = list()\n",
    "# input_names.append('TKE+IDEMIX, jayne')\n",
    "# input_names.append('TKE+IDEMIX, nycander')\n",
    "# input_names.append('TKE+IDEMIX, stormtide')\n",
    "# input_names.append('TKE')\n",
    "input_names.append('dart test')\n",
    "\n",
    "# n_cycl: which spinupcycle should be plottet if do_allcycl all spinupcycles from [1...n_cycle] are plottet, if None path is directly used\n",
    "n_cycl    = None\n",
    "do_allcycl= False\n",
    "vname     = 'temp' # if only scalar vector component v is need do: 'vec+u+v:v', when internal rotation might still be needed\n",
    "year      = [1958,1961]\n",
    "mon       = None\n",
    "day       = None\n",
    "record    = None \n",
    "box       = None\n",
    "depth     = None\n",
    "dlonlat   = 0.5   # binning resolution in deg \n",
    "# do_datavec_r2g = True  # set to False if u,v data are already in geo-coordinates\n",
    "\n",
    "#___Define Reference Data, Year, Mon ...______________________________________________\n",
    "# do anomaly plots in case ref_path is not None\n",
    "ref_path  = None # '/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke_ctrl_ck0.1/5/'\n",
    "ref_name  = None # 'TKE'\n",
    "ref_year  = None #[1979, 2019]\n",
    "ref_mon   = None\n",
    "ref_day   = None\n",
    "ref_record= None\n",
    "\n",
    "#___Define Boxregion via shape file for index compuation______________________________\n",
    "box_region = list()\n",
    "box_region.append('global')\n",
    "# box_region.append([-67, -65, -67, -55])                    # [lonmin, lonmax, latmin, latmax]\n",
    "# box_region.append([[-67, -65, -67, -55], 'Drake Passage']) # [[lonmin, lonmax, latmin, latmax], Name]\n",
    "# box_region.append('ocean_basins/Arctic_Basin.shp')\n",
    "# box_region.append('ocean_basins/Eurasian_Basin.shp')\n",
    "# box_region.append('ocean_basins/Atlantic_Basin.shp')\n",
    "# box_region.append('mpas_region/Canada_Basin.shp')\n",
    "# box_region.append('mpas_region/North_Atlantic_Ocean.shp')\n",
    "# box_region.append('mpas_region/Greenland_Sea.shp')\n",
    "# box_region.append('mpas_region/Irminger_Sea.shp')\n",
    "# box_region.append('mpas_region/Norwegian_Sea.shp')\n",
    "# box_region.append('mpas_region/Labrador_Sea.shp')\n",
    "# box_region.append('mpas_region/North_Pacific_Ocean.shp')\n",
    "# box_region.append('mpas_region/South_Pacific_Ocean.shp')\n",
    "# box_region.append('mpas_region/Southern_Ocean.shp')\n",
    "# box_region.append('mpas_region/Western_Weddell_Sea.shp')\n",
    "\n",
    "#___Define Climatology________________________________________________________________\n",
    "which_clim= 'phc3'\n",
    "clim_path = '/albedo/work/projects/p_fesom/FROM-OLLIE/FESOM2/hydrography/phc3.0/phc3.0_annual.nc'\n",
    "# clim_path = '/pool/data/AWICM/FESOM2/INITIAL/phc3.0/phc3.0_annual.nc'\n",
    "\n",
    "#___Define Colormap Parameters________________________________________________________\n",
    "# papermill doesnt like multi variable alignment in a single line\n",
    "cstr      = 'blue2red'\n",
    "cnum      = 15\n",
    "cref      = None\n",
    "crange    = None\n",
    "cmin      = None\n",
    "cmax      = None\n",
    "cfac      = None\n",
    "climit    = None\n",
    "chist     = True\n",
    "ctresh    = 0.995\n",
    "\n",
    "ref_cstr  = 'wbgyr'\n",
    "ref_cnum  = 15\n",
    "ref_cref  = None\n",
    "ref_crange= None\n",
    "ref_cmin  = None\n",
    "ref_cmax  = None\n",
    "ref_cfac  = None\n",
    "ref_climit= None\n",
    "ref_chist = True\n",
    "ref_ctresh= 0.995\n",
    "\n",
    "#___Define Plot Parameters____________________________________________________________\n",
    "ncol              = 2      # number of pannel columns in figure\n",
    "nrow              = None\n",
    "box               = [-180, 180, -90, 90]\n",
    "do_plt            = 'tcf'  # plot pcolor (tpc) or contourf (tcf)\n",
    "plt_contb         = True   # background contour line (thin)\n",
    "plt_contf         = False  # contour line of main colorbar steps \n",
    "plt_contr         = False  # contour line of reference value \n",
    "plt_contl         = False  # label contourline of main colorbar steps \n",
    "do_rescale        = None   # rescale data: None, 'log10', 'slog10', np.array(...)\n",
    "do_lsm            ='fesom' # 'fesom', 'bluemarble', 'etopo', 'stock'\n",
    "do_mesh           = False, \n",
    "mesh_opt          = dict({'color':'k', 'linewidth':0.10})\n",
    "do_enum           = False  # do enumeration of panels\n",
    "do_reffig         = True   # plot reference fig when doing anomalies \n",
    "do_clim           = False   # plot climatolgy values when doing absoluts\n",
    "ax_title          = None\n",
    "cb_label          = None\n",
    "save_dpi          = 300\n",
    "save_fmt          = ['png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start parallel dask client if do_parallel=True\n",
    "client = tpv.shortcut_setup_daskclient(client, \n",
    "                                       use_existing_client, \n",
    "                                       do_parallel, \n",
    "                                       parallel_nprc, \n",
    "                                       parallel_tmem,\n",
    "                                       threads_per_worker=parallel_nthread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#___LOAD FESOM2 MESH___________________________________________________________________________________\n",
    "mesh=tpv.load_mesh_fesom2(mesh_path, do_rot='None', do_info=True)\n",
    "\n",
    "#______________________________________________________________________________________________________\n",
    "# create input_path spinupcycle structure\n",
    "input_paths, input_names, ref_path, ref_name = tpv.shortcut_setup_pathwithspinupcycles(input_paths, input_names, ref_path, ref_name, n_cycl, do_allcycl)\n",
    "        \n",
    "#______________________________________________________________________________________________________        \n",
    "cinfo=tpv.set_cinfo(cstr, cnum, crange, cmin, cmax, cref, cfac, climit, chist, ctresh)\n",
    "ref_cinfo=None\n",
    "if (ref_path is not None): \n",
    "    if ref_year   is None: ref_year   = year\n",
    "    if ref_mon    is None: ref_mon    = mon\n",
    "    if ref_record is None: ref_record = record    \n",
    "    cinfo['cref']=0.0 \n",
    "    ref_cinfo=tpv.set_cinfo(ref_cstr, ref_cnum, ref_crange, ref_cmin, ref_cmax, ref_cref, ref_cfac, ref_climit, ref_chist, ref_ctresh)    \n",
    "\n",
    "#______________________________________________________________________________________________________    \n",
    "# concatenate ref_path and input_path together if is not None,  concatenate list = list1+list2\n",
    "input_paths, input_names = tpv.shortcut_setup_concatinputrefpath(input_paths, input_names, ref_path, ref_name)\n",
    "\n",
    "#______________________________________________________________________________________________________\n",
    "# define index regions --> reading shape files\n",
    "box = tpv.shortcut_setup_boxregion(box_region)\n",
    "\n",
    "#______________________________________________________________________________________________________\n",
    "# set predefined chunks size here! The optimized worker memory dependent chunk size is computed internally. \n",
    "# see def compute_optimal_chunks(path, client=None, varname=None, opti_dim='h', opti_chunkfrac=0.10):\n",
    "# The here presetted values are used when tpv.load_data_fesom2( ..., opti_dim=None', ...), otherwise the \n",
    "# chunks are choosen to be not larger than 10% of the worker memory tpv.load_data_fesom2( ..., \n",
    "# opti_dim='hori', opti_chunkfrax=0.1, ...). Optimized can be the horizontal, vertical or time \n",
    "# dimension opti_dim: 'h', 'v', 'hv', 'vh', 't', 'off', None\n",
    "chunks = dict({\n",
    "               'elem' : 'auto', 'nod2' : 'auto', 'edg_n': 'auto',\n",
    "               'nz1'  : 'auto', 'nz'   : 'auto', \n",
    "               'time' : 'auto', \n",
    "               }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = clock.time()\n",
    "# clean up garabage on workers before the party starts!\n",
    "if client is not None: client.run(gc.collect)\n",
    "\n",
    "data_list = list()\n",
    "#___LOAD FESOM2 DATA___________________________________________________________________________________\n",
    "for ii, (datapath, descript) in enumerate(zip(input_paths, input_names)):\n",
    "    print(ii, datapath, descript)\n",
    "    ts = clock.time()\n",
    "    #__________________________________________________________________________________________________\n",
    "    yeari, moni, dayi, recordi = year, mon, day, record\n",
    "    if (ii==0) and (ref_path != None): yeari, moni, dayi, recordi = ref_year, ref_mon, ref_day, ref_record\n",
    "\n",
    "    #____________________________________________________________________________________________________\n",
    "    # input parameter shortcut\n",
    "    input_dict = dict({'year':yeari , 'mon':moni, 'descript':descript, 'do_rot':False, \n",
    "                        'do_info':False, 'do_zarithm':None, 'do_ie2n':False, 'do_nan':False, \n",
    "                        'chunks':chunks, 'do_load':False, 'do_persist':False, 'do_parallel':do_parallel,\n",
    "                        'client':client, 'opti_dim':'v' , 'opti_chunkfrac':0.06,\n",
    "                        'do_info':False,})    \n",
    "    \n",
    "    #__________________________________________________________________________________________________\n",
    "    # load data\n",
    "    if vname=='Kv*N2':\n",
    "        data  = tpv.load_data_fesom2(mesh, datapath, vname='Kv', **input_dict)\n",
    "        data2 = tpv.load_data_fesom2(mesh, datapath, vname='N2', **input_dict)\n",
    "        data['Kv'].data = data['Kv'].data * data2['N2'].data\n",
    "        data  = data.rename(dict({'Kv':'Kv*N2'}))\n",
    "        data['Kv*N2'].attrs['units'], data['Kv*N2'].attrs['description'], data['Kv*N2'].attrs['long_name'] = '$m^2/s^3$', '(Kv)*(N^2)', '$\\\\overline{{Kv}} \\\\cdot \\\\overline{{N^2}}$'\n",
    "        del(data2)\n",
    "    elif vname=='KvN2':\n",
    "        data  = tpv.load_data_fesom2(mesh, datapath, vname='KvN2', **input_dict)\n",
    "        data['KvN2'].attrs['units'], data['KvN2'].attrs['description'], data['KvN2'].attrs['long_name'] = '$m^2/s^3$', '(Kv*N^2)', '$\\\\overline{{Kv \\\\cdot N^2}}$'\n",
    "    elif vname=='KvN2/N2':\n",
    "        data  = tpv.load_data_fesom2(mesh, datapath, vname='KvN2',  **input_dict)\n",
    "        data2 = tpv.load_data_fesom2(mesh, datapath, vname='N2'  ,  **input_dict)\n",
    "        data['KvN2'].data = data['KvN2'].data / data2['N2'].data\n",
    "        data  = data.rename(dict({'KvN2':'KvN2/N2'}))\n",
    "        data['KvN2/N2'].attrs['units'], data['KvN2/N2'].attrs['description'], data['KvN2/N2'].attrs['long_name'] = '$m^2/s$', '(Kv*N)/N2', '$\\\\overline{{Kv \\\\cdot N^2}} / \\\\overline{{N^2}}$'\n",
    "        del(data2)    \n",
    "    else:\n",
    "        data  = tpv.load_data_fesom2(mesh, datapath, vname=vname,  **input_dict)\n",
    "        \n",
    "    # check if data where loaded\n",
    "    if data is None: raise ValueError(f'data == None, data could not be readed, your path:{datapath} might be wrong!!!')\n",
    "    print(' --> elasped time to load {:s} data: {:3.2f} min.'.format(vname, (clock.time()-ts)/60  ))        \n",
    "    print(' --> data uses {:3.2f} Gb:'.format(data.nbytes/(1024**3)))\n",
    "    print('')\n",
    "\n",
    "    #__________________________________________________________________________________________________    \n",
    "    # create zonal mean reference data if given \n",
    "    ts = clock.time()\n",
    "    if (ii==0) and (ref_path != None):\n",
    "        # zmsect_ref = tpv.load_zmeantransect_fesom2Â·(mesh, data, box, do_checkbasin=False)\n",
    "        csect_ref = tpv.calc_transect_zm_mean_dask(mesh, data, box, do_parallel, parallel_nprc, \n",
    "                                                   do_lonlat='lat', dlonlat=dlonlat, client=client)\n",
    "        print(' --> elasped time to comp. csect_ref: {:3.2f} sec.'.format( (clock.time()-ts)/60  ))   \n",
    "        for ii, data_ii in enumerate(csect_ref):\n",
    "            print(' --> csect_ref[{:s}] uses {:3.2f} Mb:'.format(csect_ref[ii][list(csect_ref[ii].keys())[0]].attrs['transect_name'], csect_ref[ii].nbytes/(1024**2)))\n",
    "        print('')\n",
    "        if do_reffig: data_list.append(csect_ref) \n",
    "        del(data)    \n",
    "        continue\n",
    "    \n",
    "    # create zonal mean data \n",
    "    ts = clock.time()\n",
    "    # csect = tpv.load_zmeantransect_fesom2(mesh, data, box)\n",
    "    csect = tpv.calc_transect_zm_mean_dask(mesh, data, box, do_parallel, parallel_nprc, \n",
    "                                           do_lonlat='lat', dlonlat=dlonlat, do_checkbasin=False, client=client)\n",
    "    print(' --> elasped time to comp. csect: {:3.2f} sec.'.format( (clock.time()-ts)/60  ))   \n",
    "    for ii, data_ii in enumerate(csect):\n",
    "        print(' --> csect[{:s}] uses {:3.2f} Mb:'.format(csect[ii][list(csect[ii].keys())[0]].attrs['transect_name'], csect[ii].nbytes/(1024**2)))\n",
    "    print('')    \n",
    "    del(data)\n",
    "    if client is not None: client.run(gc.collect)\n",
    "        \n",
    "    #__________________________________________________________________________________________________    \n",
    "    # compute anomaly \n",
    "    if (ref_path != None):\n",
    "        data_list.append(tpv.do_transect_anomaly(csect, csect_ref))  \n",
    "    # compute absolute    \n",
    "    else:\n",
    "        data_list.append(csect)  \n",
    "    del(csect)\n",
    "if (ref_path != None):  del(csect_ref)\n",
    "\n",
    "#___APPEND ABS CLIMATOLOGY_____________________________________________________________________________    \n",
    "if (vname in ['temp', 'salt', 'pdens'] or 'sigma' in vname) and do_clim and (ref_path is None):\n",
    "    ts = clock.time()\n",
    "    clim_vname= vname\n",
    "    if   vname=='temp' and  which_clim.lower()=='woa18': clim_vname = 't00an1'\n",
    "    elif vname=='salt' and  which_clim.lower()=='woa18': clim_vname = 's00an1'\n",
    "    clim        = tpv.load_climatology(mesh, clim_path, clim_vname, do_load=False, \n",
    "                                       do_persist=True, do_parallel=do_parallel, chunks=chunks)\n",
    "    print(' --> elasped time to load clim: {:3.2f} min.'.format( (clock.time()-ts)/60  ))        \n",
    "    print(' --> data uses {:3.2f} Gb:'.format(clim.nbytes/(1024**3)))\n",
    "    print('')\n",
    "\n",
    "    ts = clock.time()\n",
    "    # csect_clim = tpv.load_zmeantransect_fesom2(mesh, clim, box, diagpath=input_paths[0], do_checkbasin=False)\n",
    "    csect_clim = tpv.calc_transect_zm_mean_dask(mesh, clim, box, do_parallel, parallel_nprc, \n",
    "                                                do_lonlat='lat', dlonlat=dlonlat, diagpath=input_paths[0], client=client)\n",
    "    print(' --> elasped time to comp. csect_clim: {:3.2f} sec.'.format( (clock.time()-ts)/60  ))   \n",
    "    for ii, data_ii in enumerate(csect_clim):\n",
    "        print(' --> csect_clim[{:s}] uses {:3.2f} Mb:'.format(csect_clim[ii][list(csect_clim[ii].keys())[0]].attrs['transect_name'], csect_clim[ii].nbytes/(1024**2)))\n",
    "    print('') \n",
    "    data_list.append(csect_clim)  \n",
    "    del(clim, csect_clim)\n",
    "\n",
    "print(' --> total elasped time to process data: {:3.2f} min.'.format( (clock.time()-t0)/60  ))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#___PLOT ZONMEAN TRANSECT______________________________________________________________________________\n",
    "ts = clock.time()\n",
    "nbox, ndat = len(box), len(data_list)\n",
    "if   ncol != None: \n",
    "    ncol0  = np.min([ncol,ndat])    \n",
    "    nrow0  = np.ceil(ndat/ncol0).astype('int')\n",
    "elif nrow != None: \n",
    "    nrow0  = np.min([nrow,ndat])    \n",
    "    ncol0  = np.ceil(ndat/nrow0).astype('int')\n",
    "\n",
    "for box_idx in range(nbox):\n",
    "    svname = list(data_list[0][box_idx].data_vars)[0]\n",
    "    slabel = data_list[0][box_idx][svname].attrs['str_lsave']\n",
    "    stname = data_list[0][box_idx][svname].attrs['transect_name'].replace(' ','_').lower()\n",
    "    #__________________________________________________________________________________________________\n",
    "    # do save filename path\n",
    "    spath  = save_path\n",
    "    sfpath = None\n",
    "    if spath!=None: \n",
    "        sfpath=list()\n",
    "        for sfmt in save_fmt: sfpath.append( os.path.join(spath,'{:s}_{:s}_{:s}_{:s}.{:s}'.format(svname, 'zonmean', stname ,slabel, sfmt)) )\n",
    "    if save_fname!=None: sfpath = [save_fname] # --> needed for diagrun papermille functionality\n",
    "    \n",
    "    #__________________________________________________________________________________________________\n",
    "    # do colorbar either single cbar or ref_cbar + anom_cbar\n",
    "    if (ref_path != None) and do_reffig: cb_plt, cb_plt_single, cinfo0 = [1]+[2]*(nrow0*ncol0-1), False, [ref_cinfo.copy(), cinfo.copy()]\n",
    "    else: cb_plt, cb_plt_single, cinfo0 = True, True, cinfo.copy() \n",
    "    \n",
    "    #__________________________________________________________________________________________________    \n",
    "    hfig, hax, hcb = tpv.plot_vslice(mesh, data_list, nrow=nrow0, ncol=ncol0, box_idx=box_idx, \n",
    "                                     cinfo=cinfo0, do_rescale=do_rescale,  \n",
    "                                     do_plt=do_plt, plt_contb=plt_contb, plt_contf=plt_contf, plt_contr=plt_contr, plt_contl=plt_contl, do_enum=do_enum, \n",
    "                                     ax_opt=dict({'fig_sizefac':2.0, 'cb_plt':cb_plt, 'cb_plt_single':cb_plt_single, 'cb_pos':'vertical', 'cb_h':'auto',}), # 'fs_label':14, 'fs_ticks':14, 'ax_dt':1.0}),\n",
    "                                     cbl_opt=dict(), cb_label=cb_label, cbtl_opt=dict(),\n",
    "                                     do_save=sfpath, save_dpi=save_dpi )    \n",
    "\n",
    "print(' --> elasped time to plot data: {:3.2f} min.'.format( (clock.time()-ts)/60  )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_papermill and do_parallel and client is not None: client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "py39_new",
   "language": "python",
   "name": "py39_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
