{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/albedo/home/pscholz/tripyview\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#___________________________________________________________________________________________________________________\n",
    "import os\n",
    "import tripyview as tpv\n",
    "import numpy     as np\n",
    "import xarray    as xr\n",
    "import time      as clock\n",
    "import warnings\n",
    "xr.set_options(keep_attrs=True)\n",
    "client_runs   = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFO:\n",
    "To compute especially on large unstructured grids #vertices>1M, you need to run this notebook in parallel (do_parallel=True) on several workers (parallel_nprc...is the number of dask worker that can be allocated, parallel_tmem...is the maximum available RAM that will be distributed between the dask workers). Therefor allocate a full !!! COMPUTE NODE !!! (admins might not be happy if you do this in parallel on a login node) of a HPC of your choice with as much memory (RAM) as you can get to run this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "#___Dask Client Parameters____________________________________________________________\n",
    "do_papermill      = False\n",
    "do_parallel       = False\n",
    "parallel_tnprc    = 128                          # total number of available CPUs\n",
    "parallel_nprc     = 72                           # number of dask workers\n",
    "parallel_nprc_bin = parallel_tnprc-parallel_nprc # number of processor used to parallize the binning loop\n",
    "parallel_tmem     = 200                          # max. available RAM\n",
    "\n",
    "#___Mesh Path & Save Path_____________________________________________________________\n",
    "# mesh_path ='/work/ollie/projects/clidyn/FESOM2/meshes/core2/'\n",
    "mesh_path     = '/albedo/work/user/pscholz/mesh_fesom2.0/core2_srt_dep@node/'\n",
    "save_path     = None #'~/figures/test_papermill/'\n",
    "save_fname    = None # filename from papermill come in through save_fname\n",
    "tripyrun_name      = None # papermill workflow name of notebook \n",
    "tripyrun_analysis  = None # papermill diagnostic driver\n",
    "tripyrun_spath_nb  = None # papermill path to processed notebooks\n",
    "tripyrun_spath_fig = None # papermill path to processed figures\n",
    "\n",
    "#___Data Path & Input Names___________________________________________________________\n",
    "input_paths   = list()\n",
    "input_paths.append('/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke+idemix_jayne_bin_ck0.1/5/')\n",
    "input_paths.append('/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke+idemix_nycander_bin_ck0.1/5/')\n",
    "input_paths.append('/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke+idemix_stormtide_bin_ck0.1/5/')\n",
    "input_paths.append('/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke_ctrl_ck0.1/5/')\n",
    "\n",
    "input_names   = list()\n",
    "input_names.append('TKE+IDEMIX, jayne')\n",
    "input_names.append('TKE+IDEMIX, nycander')\n",
    "input_names.append('TKE+IDEMIX, stormtide')\n",
    "input_names.append('TKE')\n",
    "\n",
    "# n_cycl: which spinupcycle should be plottet if do_allcycl all spinupcycles from [1...n_cycle] are plottet, if None path is directly used\n",
    "n_cycl       = None\n",
    "do_allcycl   = False\n",
    "which_isopyc = 36.72\n",
    "vname        = 'dflux'\n",
    "year         = [1979,2019]\n",
    "mon          = None\n",
    "day          = None\n",
    "record       = None \n",
    "box          = None\n",
    "depth        = None\n",
    "\n",
    "#___Define Reference Data, Year, Mon ...______________________________________________\n",
    "# do anomaly plots in case ref_path is not None\n",
    "ref_path  = None # '/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke_ctrl_ck0.1/5/'\n",
    "ref_name  = None # 'TKE'\n",
    "ref_year  = None #[1979, 2019]\n",
    "ref_mon   = None\n",
    "ref_day   = None\n",
    "ref_record= None\n",
    "\n",
    "#___Define Climatology________________________________________________________________\n",
    "which_clim= 'phc3'\n",
    "clim_path = '/albedo/work/projects/p_fesom/FROM-OLLIE/FESOM2/hydrography/phc3.0/phc3.0_annual.nc'\n",
    "\n",
    "#___Define Colormap Parameters________________________________________________________\n",
    "# papermill doesnt like multi variable alignment in a single line\n",
    "cstr      = 'blue2red'\n",
    "cnum      = 15\n",
    "cref      = 0\n",
    "crange    = None\n",
    "cmin      = None\n",
    "cmax      = None\n",
    "cfac      = None\n",
    "climit    = None\n",
    "chist     = True\n",
    "ctresh    = 0.995\n",
    "\n",
    "ref_cstr  = 'wbgyr'\n",
    "ref_cnum  = 15\n",
    "ref_cref  = 0\n",
    "ref_crange= None\n",
    "ref_cmin  = None\n",
    "ref_cmax  = None\n",
    "ref_cfac  = None\n",
    "ref_climit= None\n",
    "ref_chist = True\n",
    "ref_ctresh= 0.995\n",
    "\n",
    "#___Define Plot Parameters____________________________________________________________\n",
    "ncol              = 2      # number of pannel columns in figure\n",
    "nrow              = None\n",
    "proj              = 'rob' \n",
    "box               = [-180, 180, -90, 90]\n",
    "do_plt            = 'tpc'  # plot pcolor (tpc) or contourf (tcf)\n",
    "plt_contb         = True   # background contour line (thin)\n",
    "plt_contf         = True  # contour line of main colorbar steps \n",
    "plt_contr         = False  # contour line of reference value \n",
    "plt_contl         = False  # label contourline of main colorbar steps \n",
    "do_rescale        = None   # rescale data: None, 'log10', 'slog10', np.array(...)\n",
    "do_lsm            ='fesom' # 'fesom', 'bluemarble', 'etopo', 'stock'\n",
    "do_mesh           = False, \n",
    "mesh_opt          = dict({'color':'k', 'linewidth':0.10})\n",
    "do_enum           = False  # do enumeration of panels\n",
    "do_reffig         = True   # plot reference fig when doing anomalies \n",
    "do_clim           = False   # plot climatolgy values when doing absoluts\n",
    "ax_title          = None\n",
    "cb_label          = None\n",
    "save_dpi          = 300\n",
    "save_fmt          = ['png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> memory_limit: 2.778 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/albedo/home/pscholz/.conda/envs/py38/lib/python3.8/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 34573 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# start parallel dask client\n",
    "if do_parallel and not client_runs:\n",
    "    from dask.distributed import Client\n",
    "    ##import dask\n",
    "    ## dask.config.config.get('distributed').get('dashboard').update({'link':'{JUPYTERHUB_SERVICE_PREFIX}/proxy/{port}/status'})\n",
    "    client = Client(n_workers=parallel_nprc, threads_per_worker=1, memory_limit='{:3.3f} GB'.format(parallel_tmem/parallel_nprc))\n",
    "    client_runs = True\n",
    "    client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > found *.pckl file: /albedo/work/user/pscholz/mesh_fesom2.0/core2_srt_dep@node\n",
      " > load  *.pckl file: tripyview_fesom2_core2_srt_dep@node_focus0.pckl\n",
      "___FESOM2 MESH INFO________________________\n",
      " > path            = /albedo/work/user/pscholz/mesh_fesom2.0/core2_srt_dep@node\n",
      " > id              = core2_srt_dep@node\n",
      " > do rot          = None\n",
      " > [al,be,ga]      = 50, 15, -90\n",
      " > do augmpbnd     = True\n",
      " > do cavity       = False\n",
      " > do lsmask       = True\n",
      " > do earea,eresol = True, False\n",
      " > do narea,nresol = True, False\n",
      "___________________________________________\n",
      " > #node           = 126858\n",
      " > #elem           = 244659\n",
      " > #lvls           = 48\n",
      "___________________________________________\n"
     ]
    }
   ],
   "source": [
    "#___LOAD FESOM2 MESH___________________________________________________________________________________\n",
    "mesh=tpv.load_mesh_fesom2(mesh_path, do_rot='None', focus=0, do_info=True, do_pickle=True)\n",
    "\n",
    "#______________________________________________________________________________________________________\n",
    "if (n_cycl is not None): \n",
    "    cycl_s=1 if do_allcycl else n_cycl\n",
    "    #__________________________________________________________________________________________________\n",
    "    aux_path, aux_name = list(), list()\n",
    "    input_paths_old, input_names_old = input_paths, input_names\n",
    "    for ii, (ipath,iname) in enumerate(zip(input_paths,input_names)):\n",
    "        for ii_cycl in range(cycl_s, n_cycl+1):\n",
    "            aux_path.append(os.path.join(ipath,'{:d}/'.format(ii_cycl)))\n",
    "            if not do_allcycl: aux_name.append('{}'.format(iname))\n",
    "            else             : aux_name.append('{:d}) {}'.format(ii_cycl, iname))\n",
    "            print(ii, aux_path[-1],aux_name[-1])\n",
    "    input_paths, input_names = aux_path, aux_name\n",
    "    \n",
    "    #__________________________________________________________________________________________________\n",
    "    if (ref_path is not None): \n",
    "        aux_path, aux_name = list(), list()\n",
    "        ref_path_old, ref_name_old = ref_path, ref_name\n",
    "        for ii_cycl in range(cycl_s, n_cycl+1):\n",
    "            aux_path.append(os.path.join(ref_path,'{:d}/'.format(ii_cycl)))\n",
    "            if not do_allcycl: aux_name.append('{}'.format(ref_name))\n",
    "            else             : aux_name.append('{:d}) {}'.format(ii_cycl, ref_name))\n",
    "            print('R', ref_path[-1])        \n",
    "        ref_path, ref_name = aux_path, aux_name\n",
    "    del(aux_path, aux_name)       \n",
    "        \n",
    "#______________________________________________________________________________________________________        \n",
    "cinfo=tpv.set_cinfo(cstr, cnum, crange, cmin, cmax, cref, cfac, climit, chist, ctresh)\n",
    "ref_cinfo=None\n",
    "if (ref_path != None): \n",
    "    if ref_year   is None: ref_year   = year\n",
    "    if ref_mon    is None: ref_mon    = mon\n",
    "    if ref_record is None: ref_record = record    \n",
    "    cinfo['cref']=0.0 \n",
    "    ref_cinfo=tpv.set_cinfo(ref_cstr, ref_cnum, ref_crange, ref_cmin, ref_cmax, ref_cref, ref_cfac, ref_climit, ref_chist, ref_ctresh)    \n",
    "    ref_cinfo['cref']=0.0\n",
    "    \n",
    "#______________________________________________________________________________________________________    \n",
    "# concatenate list = list1+list2\n",
    "if (ref_path != None): \n",
    "    if isinstance(ref_path, list): \n",
    "        input_paths, input_names = ref_path + input_paths        , ref_name + input_names\n",
    "    else:    \n",
    "        input_paths, input_names = list([ref_path]) + input_paths, list([ref_name]) + input_names        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTENTION:\n",
    "use here the density MOC levels that are specfic for your run. They might be different from the one im using here since they might need to be customized specific to your hydrography to be able to cover the whole density range in your simulation. So check the code you are using (see. gen_modules_diag.F90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#______________________________________________________________________________________________________        \n",
    "# define density levels \n",
    "# original dima\n",
    "# std_dens=[0.0000,   30.00000, 30.55556, 31.11111, 31.36000, 31.66667, 31.91000, 32.22222, 32.46000,\n",
    "# 32.77778, 33.01000, 33.33333, 33.56000, 33.88889, 34.11000, 34.44444, 34.62000, 35.00000,\n",
    "# 35.05000, 35.10622, 35.20319, 35.29239, 35.37498, 35.41300, 35.45187, 35.52380, 35.59136,\n",
    "# 35.65506, 35.71531, 35.77247, 35.82685, 35.87869, 35.92823, 35.97566, 35.98000, 36.02115,\n",
    "# 36.06487, 36.10692, 36.14746, 36.18656, 36.22434, 36.26089, 36.29626, 36.33056, 36.36383,\n",
    "# 36.39613, 36.42753, 36.45806, 36.48778, 36.51674, 36.54495, 36.57246, 36.59500, 36.59932,\n",
    "# 36.62555, 36.65117, 36.67621, 36.68000, 36.70071, 36.72467, 36.74813, 36.75200, 36.77111,\n",
    "# 36.79363, 36.81570, 36.83733, 36.85857, 36.87500, 36.87940, 36.89985, 36.91993, 36.93965,\n",
    "# 36.95904, 36.97808, 36.99682, 37.01524, 37.03336, 37.05119, 37.06874, 37.08602, 37.10303,\n",
    "# 37.11979, 37.13630, 37.15257, 37.16861, 37.18441, 37.50000, 37.75000, 40.00000]\n",
    "\n",
    "# my density layers 2nd try\n",
    "std_dens=[ 0.00000, 29.50000, 30.00000, 30.55556, 31.11111, 31.36000, 31.66667, 31.91000, 32.22222, 32.46000,\n",
    "          32.77778, 33.01000, 33.33333, 33.56000, 33.78170, 33.79659, 33.81331, 33.83206, 33.85258, 33.87502,\n",
    "          33.88889, 33.90019, 33.92843, 33.96012, 33.99567, 34.03267, 34.07050, 34.11295, 34.16058, 34.21400,\n",
    "          34.27274, 34.33865, 34.41114, 34.47728, 34.55149, 34.62872, 34.71458, 34.81014, 34.91325, 35.02337,\n",
    "          35.13865, 35.25518, 35.37026, 35.48624, 35.58763, 35.67886, 35.76112, 35.82097, 35.87630, 35.92691,\n",
    "          35.97247, 36.02033, 36.06813, 36.11950, 36.17459, 36.23291, 36.29566, 36.36239, 36.43058, 36.50178,\n",
    "          36.57474, 36.64730, 36.71590, 36.77414, 36.82096, 36.85908, 36.89139, 36.91962, 36.94532, 36.96900,\n",
    "          36.98623, 37.00269, 37.01746, 37.03056, 37.04018, 37.05134, 37.06372, 37.07111, 37.10000, 37.25556,\n",
    "          37.41111, 37.56667, 37.72222, 37.87778, 38.03333, 38.18889, 38.34444, 38.50000, 40.00000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke+idemix_jayne_bin_ck0.1/5/ TKE+IDEMIX, jayne\n",
      " --> elasped time to load data: 0.08 min.\n",
      " --> data uses 0.17 Gb:\n",
      "\n",
      " sigma_2 = 36.72 kg/m^3\n",
      " --> elasped time to select data: 0.06 min.\n",
      " --> dflux uses 0.01 Gb:\n",
      "\n",
      "/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke+idemix_nycander_bin_ck0.1/5/ TKE+IDEMIX, nycander\n",
      " --> elasped time to load data: 0.07 min.\n",
      " --> data uses 0.17 Gb:\n",
      "\n",
      " sigma_2 = 36.72 kg/m^3\n",
      " --> elasped time to select data: 0.06 min.\n",
      " --> dflux uses 0.01 Gb:\n",
      "\n",
      "/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke+idemix_stormtide_bin_ck0.1/5/ TKE+IDEMIX, stormtide\n",
      " --> elasped time to load data: 0.07 min.\n",
      " --> data uses 0.17 Gb:\n",
      "\n",
      " sigma_2 = 36.72 kg/m^3\n",
      " --> elasped time to select data: 0.06 min.\n",
      " --> dflux uses 0.01 Gb:\n",
      "\n",
      "/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke_ctrl_ck0.1/5/ TKE\n",
      " --> elasped time to load data: 0.07 min.\n",
      " --> data uses 0.17 Gb:\n",
      "\n",
      " sigma_2 = 36.72 kg/m^3\n"
     ]
    }
   ],
   "source": [
    "#___LOAD FESOM2 DATA___________________________________________________________________________________    \n",
    "# load divergence of density class\n",
    "data_list = list()\n",
    "for datapath, descript in zip(input_paths, input_names): \n",
    "    print(datapath, descript)\n",
    "    ts = clock.time()\n",
    "    #__________________________________________________________________________________________________    \n",
    "    data = tpv.load_dmoc_data(mesh, datapath, descript, year, 'srf', std_dens, do_dflx=True, do_info=False,\n",
    "                              do_load=False, do_persist=True)\n",
    "    #__________________________________________________________________________________________________    \n",
    "    # check if data where loaded\n",
    "    if data is None: raise ValueError(f'data == None, data could not be readed, your path:{datapath} might be wrong!!!')\n",
    "    print(' --> elasped time to load data: {:3.2f} min.'.format( (clock.time()-ts)/60  ))        \n",
    "    print(' --> data uses {:3.2f} Gb:'.format(data.nbytes/(1024**3)))\n",
    "    print('')\n",
    "    \n",
    "    #___COMPUTE DIAPYCNAL VERTICAL VELOCITY____________________________________________________________\n",
    "    # finds closest index of isopycnal class\n",
    "    ts          = clock.time()\n",
    "    ndens       = len(std_dens)\n",
    "    idx_isopycn = np.argmin(np.abs(np.array(std_dens)-which_isopyc))\n",
    "    print(' sigma_2 = {:5.2f} kg/m^3'.format(std_dens[idx_isopycn]))\n",
    "\n",
    "    var        = list(data.keys())[0]\n",
    "    dflux      = -data.isel(ndens=idx_isopycn).rename({var:'dflux'})\n",
    "    del(data)\n",
    "    dflux['dflux'] = dflux['dflux'].assign_attrs({\n",
    "                    'description':'surface buoyancy forced transformations',\n",
    "                    'long_name'  :'surface buoyancy forced transformations',\n",
    "                    'short_name' :'surf. forc. transf.',\n",
    "                    'units'      :'m/s',\n",
    "                    'str_ldep'   :', $\\\\sigma_{{2}}$={:5.2f} kg$\\\\cdot$m$^{{-3}}$'.format(std_dens[idx_isopycn])})\n",
    "    dflux = dflux.load()\n",
    "    \n",
    "    # we do here a hslice plot, so proj has to be cartopy shortcut (pc,rob,nps...), not dmoc+dens\n",
    "    dflux = dflux.assign_attrs({'proj':proj})\n",
    "    \n",
    "    if (ref_path != None): \n",
    "        if ii == 0: \n",
    "            dflux_ref = dflux.copy()\n",
    "            if do_reffig: data_list.append(dflux_ref) \n",
    "        else:        \n",
    "            data_list.append( tpv.do_anomaly(dflux, dflux_ref) )\n",
    "    else:                                                   \n",
    "        data_list.append( dflux )\n",
    "    del(dflux)    \n",
    "    print(' --> elasped time to select data: {:3.2f} min.'.format( (clock.time()-ts)/60  ))        \n",
    "    print(' --> dflux uses {:3.2f} Gb:'.format(data_list[-1].nbytes/(1024**3)))\n",
    "    print('')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do regular binning of diapycnal velocities\n",
    "Rearth     = 6371e3\n",
    "dlon, dlat = 4.0, 4.0\n",
    "lon_reg    = np.arange(-180, 180+dlon, dlon)\n",
    "lat_reg    = np.arange( -90,  90+dlat, dlat)\n",
    "mlon_reg , mlat_reg = (lon_reg[1:]+lon_reg[:-1])/2.0, (lat_reg[1:]+lat_reg[:-1])/2.0\n",
    "\n",
    "#___________________________________________________________________________\n",
    "# define function for longitudinal binning --> should be possible to parallelize\n",
    "# this loop since each lon bin is independent\n",
    "def binning_over_lon_lat(lon_i_ip1, lat_reg, dlon, dlat, data):\n",
    "    #__________________________________________________________________________________________________________\n",
    "    # select elements in longitudinal bin\n",
    "    if   'nod2' in list(data.dims): dim_h = 'nod2'\n",
    "    elif 'elem' in list(data.dims): dim_h = 'elem'\n",
    "    data_lonbin = data.isel({dim_h:np.where(((data.lon-lon_i_ip1[0])*(data.lon-lon_i_ip1[1]) <= 0.))[0]}).copy()\n",
    "    \n",
    "    #__________________________________________________________________________________________________________\n",
    "    mlat         = (lat_reg[1:]+lat_reg[:-1])/2.0\n",
    "    coords       = {'lat':('nlat', mlat)}  \n",
    "    dflux_latbin = xr.DataArray(np.zeros([mlat.size, ]), dims=['nlat',])\n",
    "\n",
    "    #__________________________________________________________________________________________________________\n",
    "    # do latitudinal binning\n",
    "    rad = np.pi/180\n",
    "    for yi in range(0,mlat.size):\n",
    "        # select elements in latitudinal bin\n",
    "        dvname              = list(data_lonbin.keys())[0]\n",
    "        data_latbin         = data_lonbin.isel({dim_h:np.where(((data_lonbin.lat-lat_reg[yi])*(data_lonbin.lat-lat_reg[yi+1]) <= 0.))[0]})\n",
    "        if data_latbin.dims[dim_h]==0: continue\n",
    "        dx, dy              = np.pi*Rearth*dlon/180*np.cos((lat_reg[yi]+lat_reg[yi+1])/2.0*rad), np.pi*Rearth*dlat/180\n",
    "        data_latbin[dvname] = data_latbin[dvname]*data_latbin['w_A']\n",
    "        dflux_latbin[yi]    = data_latbin[dvname].sum(dim=dim_h)/dx/dy\n",
    "        if dflux_latbin[yi]==0.0: dflux_latbin[yi]=np.nan\n",
    "    return(dflux_latbin)    \n",
    "\n",
    "#_______________________________________________________________________________________________________________\n",
    "databin_list=list()\n",
    "for data in data_list:\n",
    "    ts        = clock.time()\n",
    "    dvname    = list(data.keys())[0]\n",
    "    #___________________________________________________________________________________________________________\n",
    "    # create xarray dataset\n",
    "    varname   = 'bin_dflux'\n",
    "    coords    = {'lon'    :(['nlon'    ], mlon_reg), 'lat'    :(['nlat'     ], mlat_reg),\n",
    "                 'lon_bnd':(['nlon_bnd'], lon_reg ), 'lat_bnd':(['nlat_bnd' ], lat_reg )}       \n",
    "    data_vars = {varname:(['nlon', 'nlat'], np.zeros([mlon_reg.size, mlat_reg.size]), data[dvname].attrs)}\n",
    "    bin_data  = xr.Dataset(data_vars=data_vars, coords=coords, attrs=data.attrs)\n",
    "\n",
    "    #___________________________________________________________________________________________________________\n",
    "    # loop over latitudinal bins\n",
    "    # serial\n",
    "    if not do_parallel:\n",
    "        for xi in range(0, mlon_reg.size):\n",
    "            bin_data[varname][xi,:] = binning_over_lon_lat([lon_reg[xi], lon_reg[xi+1]], lat_reg, dlon, dlat, data)[:]\n",
    "    else:\n",
    "        from joblib import Parallel, delayed\n",
    "        results = Parallel(n_jobs=parallel_nprc_bin)(delayed(binning_over_lon_lat)([lon_reg[xi], lon_reg[xi+1]], lat_reg, dlon, dlat, data) for xi in range(0,mlon_reg.size))\n",
    "        bin_data[varname][:,:] = xr.concat(results, dim='nlon')\n",
    "        del(results)\n",
    "    del(data)\n",
    "    bin_data[varname] = bin_data[varname].transpose('nlat','nlon')\n",
    "    databin_list.append(bin_data)\n",
    "    del(bin_data)\n",
    "    #___________________________________________________________________________________________________________\n",
    "    print(' --> elasped time to srf transformation: {:3.2f} min.'.format( (clock.time()-ts)/60  ))        \n",
    "    print(' --> dflux uses {:3.2f} Gb:'.format(databin_list[-1].nbytes/(1024**3)))\n",
    "    print('')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#___PLOT FESOM2 DATA___________________________________________________________________________________\n",
    "ndat   = len(databin_list)\n",
    "if   ncol != None: \n",
    "    ncol0  = np.min([ncol,ndat])    \n",
    "    nrow0  = np.ceil(ndat/ncol0).astype('int')\n",
    "elif nrow != None: \n",
    "    nrow0  = np.min([nrow,ndat])    \n",
    "    ncol0  = np.ceil(ndat/nrow0).astype('int')\n",
    "\n",
    "svname = list(databin_list[0].data_vars)[0]\n",
    "slabel = databin_list[0][svname].attrs['str_lsave']\n",
    "#______________________________________________________________________________________________________\n",
    "# do save filename path\n",
    "spath  = save_path\n",
    "sfpath = None\n",
    "if spath!=None: \n",
    "    sfpath=list()\n",
    "    for sfmt in save_fmt: sfpath.append( os.path.join(spath,'{:s}_{:s}_{:s}.{:s}'.format(svname, proj, slabel, sfmt)) )\n",
    "if save_fname!=None: sfpath = [save_fname] # --> needed for diagrun papermille functionality\n",
    "\n",
    "#______________________________________________________________________________________________________\n",
    "# do colorbar either single cbar or ref_cbar + anom_cbar\n",
    "if (ref_path != None) and do_reffig: cb_plt, cb_plt_single, cinfo0 = [1]+[2]*(nrow0*ncol0-1), False, [ref_cinfo.copy(), cinfo.copy()]\n",
    "else: cb_plt, cb_plt_single, cinfo0 = True, True, cinfo.copy() \n",
    "\n",
    "#__________________________________________________________________________________________________\n",
    "hfig, hax, hcb = tpv.plot_hslice(mesh, databin_list, cinfo=cinfo0, box=box, nrow=nrow0, ncol=ncol0, proj=proj, do_rescale=do_rescale,  \n",
    "                                do_plt=do_plt, plt_contb=plt_contb, plt_contf=plt_contf, plt_contr=plt_contr, plt_contl=plt_contl, do_enum=do_enum, \n",
    "                                ax_opt=dict({'fig_sizefac':2.0, 'cb_plt':cb_plt, 'cb_plt_single':cb_plt_single, 'cb_pos':'vertical', 'cb_h':'auto',}), # 'fs_label':14, 'fs_ticks':14, 'ax_dt':1.0}),\n",
    "                                cbl_opt=dict(), cb_label=cb_label, cbtl_opt=dict(),\n",
    "                                do_save=sfpath, save_dpi=save_dpi)  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
