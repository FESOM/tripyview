{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# get_ipython().magic('matplotlib notebook')\n",
    "# get_ipython().magic('matplotlib inline')\n",
    "# get_ipython().magic('load_ext autoreload')\n",
    "# get_ipython().magic('autoreload 2')\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "#___________________________________________________________________________________________________________________\n",
    "import os\n",
    "import tripyview as tpv\n",
    "import shapefile as shp\n",
    "import numpy     as np\n",
    "import xarray    as xr\n",
    "import time      as clock\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "xr.set_options(keep_attrs=True)\n",
    "do_parallel       = False\n",
    "parallel_nprc     = 48   # number of dask workers\n",
    "parallel_nprc_bin = 10   # number of processor used to parallize the binning loop\n",
    "parallel_tmem     = 200  # max. available RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFO:\n",
    "To compute especially on large unstructured grids #vertices>1M, you need to run this notebook in parallel (do_parallel=True) on several workers (parallel_nprc...is the number of dask worker that can be allocated, parallel_tmem...is the maximum available RAM that will be distributed between the dask workers). Therefor allocate a full !!! COMPUTE NODE !!! (admins might not be happy if you do this in parallel on a login node) of a HPC of your choice with as much memory (RAM) as you can get to run this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_parallel:\n",
    "    from dask.distributed import Client\n",
    "    # from dask.diagnostics import ProgressBar\n",
    "    import dask\n",
    "    print(' --> memory_limit: {:3.3f} GB'.format(parallel_tmem/(parallel_nprc)))\n",
    "    ## dask.config.config.get('distributed').get('dashboard').update({'link':'{JUPYTERHUB_SERVICE_PREFIX}/proxy/{port}/status'})\n",
    "    client = Client(n_workers=parallel_nprc, threads_per_worker=1, memory_limit='{:3.3f} GB'.format(parallel_tmem/parallel_nprc))\n",
    "    client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "mesh_path = '/albedo/work/user/pscholz/mesh_fesom2.0/core2_srt_dep@node/'\n",
    "# mesh_path = '/albedo/work/user/pscholz/mesh_fesom2.0/dart_test/'\n",
    "save_path = None #'~/figures/test_papermill/'\n",
    "save_fname= None\n",
    "\n",
    "#_____________________________________________________________________________________\n",
    "which_cycl= None #5 # set None --> take path as in input_paths otherwise add dir of cycle\n",
    "which_mode= 'mhflx' # global heat flux\n",
    "\n",
    "#_____________________________________________________________________________________\n",
    "input_paths= list()\n",
    "# input_paths.append('/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke+idemix_jayne_bin_ck0.1/5/')\n",
    "# input_paths.append('/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke+idemix_nycander_bin_ck0.1/')\n",
    "# input_paths.append('/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke+idemix_stormtide_bin_ck0.1/')\n",
    "input_paths.append('/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke_ctrl_ck0.1/5/')\n",
    "# input_paths.append('/albedo/work/user/pscholz/results/dart_linfs_pc0_ctrl_1/1/')\n",
    "\n",
    "input_names= list()\n",
    "# input_names.append('TKE+IDEMIX, jayne')\n",
    "# input_names.append('TKE+IDEMIX, nycander')\n",
    "# input_names.append('TKE+IDEMIX, stormtide')\n",
    "input_names.append('TKE')\n",
    "\n",
    "vname     = 'mhflx'\n",
    "year      = [1979,2019]\n",
    "mon, day, record, box, depth = None, None, None, None, None\n",
    "\n",
    "#_____________________________________________________________________________________\n",
    "box_region = list()\n",
    "box_region.append('global')\n",
    "box_region.append('moc_basins/Atlantic_MOC.shp')\n",
    "box_region.append('moc_basins/IndoPacific_MOC.shp')\n",
    "# box_region.append('ocean_basins/Atlantic_Basin.shp')\n",
    "# box_region.append('ocean_basins/Pacific_Basin.shp')\n",
    "\n",
    "#_____________________________________________________________________________________\n",
    "# do anomaly plots in case ref_path is not None\n",
    "ref_path  = None #'/home/ollie/pscholz/results/trr181_tke_ctrl_ck0.1/' # None\n",
    "ref_name  = None # 'TKE, ck=0.1' # None\n",
    "ref_year  = None # [2009,2019]\n",
    "ref_mon, ref_day, ref_record = None, None, None\n",
    "\n",
    "#_____________________________________________________________________________________\n",
    "cstr, cnum = 'blue2red', 20\n",
    "cref, crange, cmin, cmax, cfac, climit = None, None, None, None, None, None\n",
    "chist, ctresh = True, 0.995\n",
    "\n",
    "#_____________________________________________________________________________________\n",
    "which_dpi = 300\n",
    "do_allcycl= False\n",
    "use_advflx= True  # True: compute hflux from u*t and v*t, False: compute hflx from u, v, temp,+(u_bolus, v_bolus)\n",
    "use_bolusv= True  # True: include bolus velocity\n",
    "do_edgevec_r2g = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > found *.pckl file: /albedo/work/user/pscholz/mesh_fesom2.0/core2_srt_dep@node\n",
      " > load  *.pckl file: tripyview_fesom2_core2_srt_dep@node_focus0.pckl\n",
      "___FESOM2 MESH INFO________________________\n",
      " > path            = /work/ollie/pscholz/mesh_fesom2.0/core2_srt_dep@node\n",
      " > id              = core2_srt_dep@node\n",
      " > do rot          = None\n",
      " > [al,be,ga]      = 50, 15, -90\n",
      " > do augmpbnd     = True\n",
      " > do cavity       = False\n",
      " > do lsmask       = True\n",
      " > do earea,eresol = True, False\n",
      " > do narea,nresol = True, False\n",
      "___________________________________________\n",
      " > #node           = 126858\n",
      " > #elem           = 244659\n",
      " > #lvls           = 48\n",
      "___________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/albedo/home/pscholz/tripyview/tripyview/sub_mesh.py:139: UserWarning: Unpickling a shapely <2.0 geometry object. Please save the pickle again; shapely 2.1 will not have this compatibility.\n",
      "  mesh = pickle.load(fid)\n"
     ]
    }
   ],
   "source": [
    "#___LOAD FESOM2 MESH___________________________________________________________________________________\n",
    "mesh=tpv.load_mesh_fesom2(mesh_path, do_rot='None', focus=0, do_info=True, do_pickle=True,\n",
    "                      do_earea=True, do_narea=True)\n",
    "\n",
    "#______________________________________________________________________________________________________\n",
    "if (which_cycl is not None) and (which_cycl != \"None\"): \n",
    "    #__________________________________________________________________________________________________\n",
    "    if do_allcycl: cycl_s=1\n",
    "    else         : cycl_s=which_cycl    \n",
    "    \n",
    "    #__________________________________________________________________________________________________\n",
    "    aux_path, aux_name = list(), list()\n",
    "    input_paths_old, input_names_old = input_paths, input_names\n",
    "    for ii, (ipath,iname) in enumerate(zip(input_paths,input_names)):\n",
    "        for ii_cycl in range(cycl_s, which_cycl+1):\n",
    "            # input_paths[ii] = os.path.join(ipath,'{:d}/'.format(which_cycl))\n",
    "            aux_path.append(os.path.join(ipath,'{:d}/'.format(ii_cycl)))\n",
    "            if not do_allcycl: aux_name.append('{}'.format(iname))\n",
    "            else             : aux_name.append('{:d}) {}'.format(ii_cycl, iname))\n",
    "            print(ii, aux_path[-1],aux_name[-1])\n",
    "    input_paths, input_names = aux_path, aux_name\n",
    "    \n",
    "    #__________________________________________________________________________________________________\n",
    "    if (ref_path is not None) and (ref_path != \"None\"): \n",
    "        aux_path, aux_name = list(), list()\n",
    "        ref_path_old, ref_name_old = ref_path, ref_name\n",
    "        for ii_cycl in range(cycl_s, which_cycl+1):\n",
    "            #ref_path = os.path.join(ref_path,'{:d}/'.format(which_cycl))\n",
    "            aux_path.append(os.path.join(ref_path,'{:d}/'.format(ii_cycl)))\n",
    "            if not do_allcycl: aux_name.append('{}'.format(ref_name))\n",
    "            else             : aux_name.append('{:d}) {}'.format(ii_cycl, ref_name))\n",
    "            print('R', ref_path[-1])        \n",
    "        ref_path, ref_name = aux_path, aux_name\n",
    "    del(aux_path, aux_name)\n",
    "    \n",
    "#______________________________________________________________________________________________________\n",
    "cinfo=tpv.set_cinfo(cstr, cnum, crange, cmin, cmax, cref, cfac, climit, chist, ctresh)\n",
    "ref_cinfo=None\n",
    "if (ref_path is not None) and (ref_path != \"None\"): \n",
    "    if ref_year   is None: ref_year   = year\n",
    "    if ref_mon    is None: ref_mon    = mon\n",
    "    if ref_record is None: ref_record = record\n",
    "    cinfo['cref']=0.0 \n",
    "    ref_cinfo=tpv.set_cinfo(ref_cstr, ref_cnum, ref_crange, ref_cmin, ref_cmax, ref_cref, ref_cfac, ref_climit, ref_chist, ref_ctresh)\n",
    "else:\n",
    "    do_reffig=False\n",
    "\n",
    "#______________________________________________________________________________________________________    \n",
    "# concatenate list = list1+list2\n",
    "if (ref_path is not None) and (ref_path != \"None\"): \n",
    "    if isinstance(ref_path, list): \n",
    "        input_paths, input_names = input_paths + ref_path, input_names + ref_name \n",
    "    else:    \n",
    "        input_paths, input_names = input_paths + list([ref_path]), input_names + list([ref_name])\n",
    "        \n",
    "#______________________________________________________________________________________________________\n",
    "box = list()\n",
    "shp_path = os.path.join(tpv.__path__[0],'shapefiles/')\n",
    "for region in box_region:\n",
    "    if region == 'global' or isinstance(region,list): box.append(region)\n",
    "    else: box.append(shp.Reader(os.path.join(shp_path,region)))        \n",
    "\n",
    "#______________________________________________________________________________________________________\n",
    "# use number of worker dependent chunk size for nodes and elements\n",
    "chunks = dict({'time' : 'auto', \n",
    "               'elem' : 'auto', \n",
    "               'nod2' : 'auto', \n",
    "               'nz1'  : 'auto', \n",
    "               'nz'   : 'auto',\n",
    "               'edg_n': 'auto'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:     (n2: 2, edg_n: 371644)\n",
      "Dimensions without coordinates: n2, edg_n\n",
      "Data variables:\n",
      "    edges       (n2, edg_n) int32 0 0 0 1 1 ... 126830 126832 126856 126857\n",
      "    edge_tri    (n2, edg_n) int32 0 3 1 2 6 5 4 7 6 ... -1 -1 -1 -1 -1 -1 -1 -1\n",
      "    edge_x      (n2, edg_n) float32 -179.7 -179.7 -179.7 ... 175.3 178.5 179.4\n",
      "    edge_y      (n2, edg_n) float32 -77.94 -77.94 -77.94 ... -77.98 -78.02\n",
      "    edge_mx     (edg_n) float32 -0.2757 0.1135 -179.4 ... 175.0 178.2 179.0\n",
      "    edge_my     (edg_n) float32 -77.88 -77.78 -77.84 ... -77.49 -77.9 -78.0\n",
      "    edge_dx_lr  (n2, edg_n) float32 -2.484e+03 -7.361e+03 -4.272e+03 ... 0.0 0.0\n",
      "    edge_dy_lr  (n2, edg_n) float32 -5.209e+03 -867.5 8.242e+03 ... 0.0 0.0 0.0\n",
      " --> elasped time: 0.038156028588612875 min.\n",
      " --> mdiag uses 0.02 Gb:\n"
     ]
    }
   ],
   "source": [
    "ts = clock.time()\n",
    "datapath = input_paths[0]\n",
    "#___________________________________________________________________________\n",
    "# load information about edges \n",
    "fname = 'fesom.mesh.diag.nc'\n",
    "# check for directory with diagnostic file\n",
    "if   os.path.isfile( os.path.join(datapath, fname) ): \n",
    "    dname = datapath\n",
    "elif os.path.isfile( os.path.join( os.path.join(os.path.dirname(os.path.normpath(datapath)),'1/'), fname) ): \n",
    "    dname = os.path.join(os.path.dirname(os.path.normpath(datapath)),'1/')\n",
    "elif os.path.isfile( os.path.join(mesh.path,fname) ): \n",
    "    dname = mesh.path\n",
    "else:\n",
    "    raise ValueError('could not find directory with...mesh.diag.nc file')    \n",
    "\n",
    "#___________________________________________________________________________\n",
    "# set specfic type when loading --> #convert to specific precision\n",
    "# drop unnecessary variables:  Based on the documentation (but a bit hidden), \n",
    "# the \"data_vars=\" argument only works with Python 3.9.\n",
    "from functools import partial\n",
    "var_keep = ['edges', 'edge_tri', 'edge_cross_dxdy']\n",
    "def _preprocess(x):\n",
    "    for var in list(x.keys()):\n",
    "        if var not in var_keep: \n",
    "            x = x.drop_vars(var)\n",
    "            continue\n",
    "        if x[var].dtype=='float64': x[var] = x[var].astype('float32')\n",
    "    return x\n",
    "partial_func = partial(_preprocess)\n",
    "\n",
    "#___________________________________________________________________________\n",
    "# load diag file --> apply drop variables by preprocessor function\n",
    "mdiag = xr.open_mfdataset(os.path.join(dname,fname), parallel=False, \n",
    "                             chunks=dict({'edg_n':'auto'}), engine='netcdf4', \n",
    "                             preprocess=partial_func)\n",
    "mdiag = mdiag.drop_vars(list(mdiag.coords)).unify_chunks()\n",
    "\n",
    "# node indices of edge points [2 x n2ded]--> indices in python start with 0\n",
    "mdiag['edges']    = mdiag['edges']-1\n",
    "# element indices of triangles that are left and right of edg: [2 x n2ded]\n",
    "# --> indices in python start with 0\n",
    "mdiag['edge_tri'] = mdiag['edge_tri']-1\n",
    "\n",
    "# Suppress the specific warning about sending large graphs\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Sending large graph of size\")\n",
    "\n",
    "#___________________________________________________________________________\n",
    "# add edge coordinates and edge mid point coordinates\n",
    "set_mdiag_chunk = dict({'edg_n': mdiag.chunksizes['edg_n']})\n",
    "mdiag = mdiag.assign(edge_x = xr.DataArray(mesh.n_x[mdiag.edges], dims=['n2','edg_n']).astype('float32').chunk(set_mdiag_chunk))\n",
    "mdiag = mdiag.assign(edge_y = xr.DataArray(mesh.n_y[mdiag.edges], dims=['n2','edg_n']).astype('float32').chunk(set_mdiag_chunk))\n",
    "# mdiag = mdiag.drop_vars('edges')\n",
    "\n",
    "mdiag = mdiag.assign(edge_mx = mdiag.edge_x.sum(dim='n2')/2.0)\n",
    "mdiag = mdiag.assign(edge_my = mdiag.edge_y.sum(dim='n2')/2.0)\n",
    "\n",
    "#___________________________________________________________________________\n",
    "# Be sure that the edge_cross_dxdy variable is in the same rotational frame as your velocities. By default\n",
    "# edge_cross_dxdy is in rotated coordinates. So if you velocities are also in rotated coordinates things are fine.\n",
    "# If your velocities should be in geo coordinates than edge_cross_dxdy needs to be rotated as well into geo\n",
    "# coordinates (do_edgevec_r2g=True)\n",
    "if (do_edgevec_r2g):\n",
    "    mdiag.edge_cross_dxdy[0,:], mdiag.edge_cross_dxdy[1,:] = tpv.vec_r2g(mesh.abg, mdiag.edge_mx, mdiag.edge_my, \n",
    "                                                                         mdiag.edge_cross_dxdy[0,:], mdiag.edge_cross_dxdy[1,:], \n",
    "                                                                         gridis='geo', do_info=False )\n",
    "    mdiag.edge_cross_dxdy[2,:], mdiag.edge_cross_dxdy[3,:] = tpv.vec_r2g(mesh.abg, mdiag.edge_mx, mdiag.edge_my, \n",
    "                                                                         mdiag.edge_cross_dxdy[2,:], mdiag.edge_cross_dxdy[3,:], \n",
    "                                                                         gridis='geo', do_info=False )\n",
    "\n",
    "#___________________________________________________________________________\n",
    "set_mdiag_chunk = dict({'edg_n': mdiag.chunksizes['edg_n'], 'n2':mdiag.chunksizes['n2']})\n",
    "# dx & dy of left triangle --> norm vector \n",
    "mdiag = mdiag.assign(edge_dxdy_l=xr.DataArray( np.array([ mdiag.edge_cross_dxdy[1,:], \n",
    "                                                         -mdiag.edge_cross_dxdy[0,:]]), dims=['n2', 'edg_n']).chunk(set_mdiag_chunk))\n",
    "# dx & dy of right triangle --> norm vector \n",
    "mdiag = mdiag.assign(edge_dxdy_r=xr.DataArray( np.array([-mdiag.edge_cross_dxdy[3,:], \n",
    "                                                          mdiag.edge_cross_dxdy[2,:]]), dims=['n2', 'edg_n']).chunk(set_mdiag_chunk))\n",
    "# if boundarie edge --> right triangle doesnot exist --> therefor dx,dy is zero there\n",
    "mdiag.edge_dxdy_r[:, mdiag.edge_tri[1,:]<0]=0. \n",
    "\n",
    "#___________________________________________________________________________\n",
    "# restructure edge_dxdy_l & edge_dxdy_r into edge_dx_lr0 & edge_dx_lr1\n",
    "mdiag = mdiag.assign(edge_dx_lr=xr.DataArray( np.array([ mdiag.edge_dxdy_l[0,:], \n",
    "                                                         mdiag.edge_dxdy_r[0,:]]), dims=['n2', 'edg_n']).chunk(set_mdiag_chunk))\n",
    "# dx & dy of right triangle --> norm vector \n",
    "mdiag = mdiag.assign(edge_dy_lr=xr.DataArray( np.array([ mdiag.edge_dxdy_l[1,:], \n",
    "                                                         mdiag.edge_dxdy_r[1,:]]), dims=['n2', 'edg_n']).chunk(set_mdiag_chunk))\n",
    "\n",
    "#___________________________________________________________________________\n",
    "# now drop edge_cross_dxdy variable dont need anymore\n",
    "mdiag = mdiag.drop_vars(['edge_cross_dxdy', 'edge_dxdy_l', 'edge_dxdy_r']) #\n",
    "mdiag = mdiag.load()\n",
    "\n",
    "#___________________________________________________________________________\n",
    "warnings.resetwarnings()\n",
    "print(mdiag)\n",
    "print(' --> elasped time: {} min.'.format( (clock.time()-ts)/60  ))\n",
    "print(' --> mdiag uses {:3.2f} Gb:'.format(mdiag.nbytes/(1024**3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/albedo/work/projects/p_fesom/pscholz/project_TRR181/trr181_tke_ctrl_ck0.1/5/ TKE\n",
      " --> elapsed time to load, prepare u,v, temp data: 0.35 min.\n",
      " --> data_uvt uses 0.28 Gb:\n"
     ]
    }
   ],
   "source": [
    "#___LOAD FESOM2 DATA___________________________________________________________________________________\n",
    "# load t*u and t*v together\n",
    "hflx_list = list()\n",
    "for datapath, descript in zip(input_paths, input_names): \n",
    "    print(datapath, descript)\n",
    "    ts = clock.time()\n",
    "    if use_advflx:\n",
    "        #__________________________________________________________________________________________________\n",
    "        data = tpv.load_data_fesom2(mesh, datapath, vname='vec+tu+tv', year=year, descript=descript, \n",
    "                                    do_info=False, do_vecrot=False, do_zarithm='None', do_ie2n=False,\n",
    "                                    chunks=chunks, do_compute=False, do_load=True, do_persist=False, )\n",
    "        \n",
    "        #__________________________________________________________________________________________________    \n",
    "        # check if data where loaded\n",
    "        if data is None: raise ValueError(f'data == None, data could not be readed, your path:{datapath} might be wrong!!!')\n",
    "\n",
    "        #__________________________________________________________________________________________________\n",
    "        # combine edge diagnostic and u,v data on edge_tri indices --> now dataset in context of edge\n",
    "        # information --> to reindex here towards edges the data need to be unchunked, therefor you putted\n",
    "        # do_load=True\n",
    "        data = xr.merge([mdiag, data.tu[mdiag.edge_tri,:], data.tv[mdiag.edge_tri,:]])\n",
    "        \n",
    "    else:\n",
    "        #__________________________________________________________________________________________________\n",
    "        # load velocities\n",
    "        data = tpv.load_data_fesom2(mesh, datapath, vname='vec+u+v', year=year, descript=descript, \n",
    "                                    do_info=False, do_vecrot=False, do_zarithm='None', do_ie2n=False, chunks=chunks, \n",
    "                                    do_compute=False, do_load=True, do_persist=False, )\n",
    "        \n",
    "        #__________________________________________________________________________________________________    \n",
    "        # check if data where loaded\n",
    "        if data is None: raise ValueError(f'data == None, data could not be readed, your path:{datapath} might be wrong!!!')\n",
    "    \n",
    "        #__________________________________________________________________________________________________    \n",
    "        if use_bolusv:\n",
    "            data['u'].data = data['u'].data + tpv.load_data_fesom2(mesh, datapath, vname='bolus_u', year=year, descript=descript, \n",
    "                                                                   do_info=False, do_vecrot=False, do_zarithm='None', do_ie2n=False,\n",
    "                                                                   chunks=chunks, do_compute=False, do_load=True, do_persist=False)['bolus_u'].data\n",
    "            data['v'].data = data['v'].data + tpv.load_data_fesom2(mesh, datapath, vname='bolus_v', year=year, descript=descript, \n",
    "                                                                   do_info=False, do_vecrot=False, do_zarithm='None', do_ie2n=False,\n",
    "                                                                   chunks=chunks, do_compute=False, do_load=True, do_persist=False)['bolus_v'].data\n",
    "        \n",
    "        #__________________________________________________________________________________________________\n",
    "        # load temperature\n",
    "        datat = tpv.load_data_fesom2(mesh, datapath, vname='temp', year=year, descript=descript, \n",
    "                                     do_info=False, do_vecrot=False, do_zarithm='None', do_ie2n=False,\n",
    "                                     chunks=chunks, do_compute=False, do_load=True, do_persist=False)\n",
    "        # kick everything out that is not needed anymore\n",
    "        datat = datat.drop_vars(list(datat.coords))\n",
    "        \n",
    "        #__________________________________________________________________________________________________\n",
    "        # combine edge diagnostic and u,v data on edge_tri indices --> now dataset in context of edge\n",
    "        # information --> to reindex here towards edges the data need to be unchunked, therefor you putted\n",
    "        # do_load=True\n",
    "        data  = xr.merge([mdiag, data.u[mdiag.edge_tri,:], data.v[mdiag.edge_tri,:], datat.temp[mdiag.edges,:]])\n",
    "        del(datat)\n",
    "\n",
    "    # kick everything out that is not needed anymore,\n",
    "    # keep in mind that lon,lat here is the lon,lat position of the left and right triangle !!!\n",
    "    data  = data.drop_vars(['elemi', 'elemiz', 'w_A', 'nz1', 'nzi', 'lon', 'lat', 'edge_mx']) #'edge_tri', 'edges', \n",
    "        \n",
    "    # add vertical layer thickness as coordinate \n",
    "    data  = data.assign_coords(dz = xr.DataArray(np.diff(-mesh.zlev).astype('float32'), dims='nz1'))\n",
    "    data  = data.set_coords(['edge_y', 'edge_dx_lr', 'edge_dy_lr', 'edge_tri', 'edge_my', 'edge_x', 'edges'])\n",
    "        \n",
    "    #__________________________________________________________________________________________________\n",
    "    print(' --> elapsed time to load, prepare u,v, temp data: {:3.2f} min.'.format( (clock.time()-ts)/60  ))        \n",
    "    print(' --> data_uvt uses {:3.2f} Gb:'.format(data.nbytes/(1024**3)))\n",
    "\n",
    "    #__________________________________________________________________________________________________\n",
    "    # compute meridional heat flux from t*v by binning\n",
    "    ts = clock.time()\n",
    "    hflx_list.append(tpv.calc_mhflx_box_fast(mesh, data, box, dlat=1.0, do_parallel=do_parallel, n_workers=parallel_nprc_bin, do_info=False))\n",
    "    print(' --> elapsed time to comp. merid. hflux: {:3.2f} min.'.format( (clock.time()-ts)/60  ))   \n",
    "    del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#___PLOT FESOM2 DATA___________________________________________________________________________________\n",
    "spath  = save_path\n",
    "sname  = list(hflx_list[0][0].keys())[0]\n",
    "slabel = hflx_list[0][0][sname].attrs['str_lsave']\n",
    "if spath is not None: spath  = os.path.join(spath,'{}_{}_{}.png'.format(which_mode, sname, slabel)) \n",
    "if save_fname is not None: spath = save_fname\n",
    "fig, ax = tpv.plot_mhflx(hflx_list, input_names, sect_name=None, figsize=[7*2, 3.5*2], \n",
    "                         do_allcycl=False, do_save=spath, save_dpi=which_dpi,)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
